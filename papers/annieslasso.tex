%% This file is part of the Annie's Lasso project.
%% Copyright 2015 the authors.  All rights reserved.

% To-Do
% -----
% - make full outline
% - get to zeroth draft
% - audit for usage of vector and list
% - search for all occurrences of DWH, AC, or MKN in the text and fix them.
% - audit 'test-set star', 'training-set star', etc. should X-set be hyphenated in these circumstances?
% - audit: sparsity definition

% Style Notes
% -----------
% - Use \acronym{NASA} for acronyms like NASA.
% - The label list \ell is a *list*; the vectorization v(ell) is a *vector*.

\documentclass[12pt,preprint]{aastex}
\usepackage{amsmath,amssymb}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\TheCannon}{\project{The~Cannon}}
\newcommand{\tc}{\TheCannon}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\sdss}{\project{\acronym{SDSS-IV}}}
\newcommand{\sdssiii}{\project{\acronym{SDSS-III}}}
\newcommand{\apogee}{\project{\acronym{APOGEE}}}
\newcommand{\aspcap}{\project{\acronym{ASPCAP}}}
\newcommand{\dr}{\acronym{DR12}}
\newcommand{\logg}{\log g}
\newcommand{\mh}{\mathrm{[M/H]}}
\newcommand{\Teff}{T_{\mathrm{eff}}}
\newcommand{\Dvector}[1]{\boldsymbol{#1}}
\newcommand{\vectheta}{\Dvector{\theta}}
\newcommand{\vecv}{\Dvector{v}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}\,}

\begin{document}

\title{\textsl{The Cannon 2:} A data-driven model of stellar spectra \\
       for detailed chemical abundance analyses}
\author{Andrew~R.~Casey\altaffilmark{1},
        David~W.~Hogg\altaffilmark{2,3,4,5},
        Melissa~Ness\altaffilmark{5},\\
        Hans-Walter~Rix\altaffilmark{5},
    and~Gerry~Gilmore\altaffilmark{1}}
\altaffiltext{1}{Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge CB3~0HA, UK}    
\altaffiltext{2}{Simons Center for Data Analysis, 160 Fifth Avenue, 7th Floor, New York, NY 10010, USA}
\altaffiltext{3}{Center for Cosmology and Particle Physics, Department of Physics,
  New York University, 4 Washington Pl., room 424, New York, NY, 10003, USA}
\altaffiltext{4}{Center for Data Science, New York University, 726 Broadway, 7th Floor, New York, NY 10003, USA}
\altaffiltext{5}{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\email{arc@ast.cam.ac.uk}


\begin{abstract}
% Context
In previous work it has been shown that it is possible to train a generative
probabilistic model for stellar spectra using a training set of stars, each 
labeled with known physical attributes ($\Teff$, $\logg$, $\mh$), and then use 
that model to infer labels for unlabeled stars, even stars with lower 
signal-to-noise observations \citep{tc}.
% Aims
Here we ask whether this is possible when the dimensionality of the chemical
abundance space is large (15 abundances: C, N, O, Na, Mg, Al, Si, S, K, Ca, Ti, 
V, Mn, Fe, Ni) and the model is non-linear in its response to abundance and 
parameter changes.
% Methods
We adopt ideas from compressed sensing to limit overall model complexity (number
of non-zero model parameters) while retaining model freedom.  The training set 
consists of 12,681 red-giant stars with high signal-to-noise spectroscopic 
observations and stellar parameters and abundances taken from the \apogee\ 
Survey.
% Results
We find that we can successfully train and use a model with 17 stellar labels.
Validation shows that the model does a good job of inferring all 17 labels (typical precision is 0.04~dex), even
when we degrade the signal-to-noise by discarding some of
the spectroscopic observing time.  The model dependencies make sense: the 
derivatives of the spectral mean model with respect to abundances correlate well
with known atomic lines, and we are able to identify elements belonging to 
atomic lines that were previously unknown.  We deliver open-source code and also
stellar parameters and 15 abundances for a set of 98,462 stars.
\end{abstract}



\section{Introduction}
The detailed surface abundances of a star reflect its formation environment and
the supernova(e) that preceded it.  Perturbations in the properties of 
either can produce subtle chemical abundance differences on successive
generations of stars.  This chemical abundance signature largely remains 
unchanged in the photosphere throughout a star's lifetime, thereby providing a 
fossilised record of the gas composition during formation and an imprinted 
signature of stars that preceded it.  The variation between these signatures is 
small, implying that it is difficult to distinguish subtle chemical patterns 
from different formation sites.  However if they were distinguishable, a 
sufficiently large collection of precise stellar abundances would unambiguously
unravel the complete chemical evolution of the Milky Way.


There are some exceptions to this line of reasoning: the abundances of some
elements can (and do) change throughout a star's lifetime.  However these are
subtle signatures that are reflective of intriguing astrophysical phenomena:
atomic diffusion, evolutionary mixing, the presence (or accretion) of 
exoplanets, material transfer from a stellar binary companion, and countless
other scenarios.  In the same way that precise abundances are required to 
identify unique chemical formation signatures, precise abundances are necessary
to distinguish minute evolutionary or environment differences between subsets of
stars.  In these scenarios we argue most astrophysicists care less about
accuracy; it is the abundance \emph{precision} that imparts discovery and 
understanding of these astrophysical phenomena.  Indeed, in most studies of 
stellar astrophysics \textit{an increase in precision yields far more 
astrophysical insight than a comparable increase in accuracy}.


For these reasons a comprehensive catalog of precise detailed chemical 
abundances would revolutionise our understanding of planetary, stellar, and 
galactic astrophysics.  This goal has only recently become feasible given the 
increasing volume and quality of stellar spectra obtained in the last decade.  
Specifically, large surveys are obtaining high-resolution 
($\mathcal{R} \gtrsim 20,000$), high signal-to-noise (S/N) ratio spectra for
$\sim10^5$---$10^6$ stars across all components of the Galaxy.  This is a sharp
relative increase in data volume, which has mandated the automation of spectral
analysis, and encouraged dozens of groups to produce bespoke pipelines.


Most automated pipelines have grown from classical, manual methods.  That is to
say, there has been relatively little work on unconventional methods to analyse
spectra.  Spectroscopists have sought to hard-code their experience (or 
subjectivity), with arbitrary heuristics enforced for wavelength masks, 
convergence criteria, and similar analysis issues.  Most of these decisions are 
based on optimizing the resultant accuracy for a few stars with good 
quality data (e.g., high S/N).  As a result, these heuristics are frequently 
incompatible for data with more modest (and representative) S/N ratios.  
Therefore, for most stars in the sample, it can be shown from repeat or blind 
experiments that that individual pipelines yield demonstrably imprecise 
abundances for data with more typical S/N ratios.  Moreover, results from 
individual pipelines vary significantly with respect to each other, with 
abundance differences an order of magnitude larger than any of the 
aforementioned astrophysical signatures of interest.  Thus, while there has been
substantial effort in automating classical analysis techniques, they are 
generally imprecise at modest S/N ratios, and frequently yield inaccurate 
results.


Considerable efforts have also been spent on improving the accuracy of physical
models.  Indeed it is impossible to measure physical properties of stars (or 
their chemical abundances) accurately without accurate physical models of 
stellar spectra.  However, physics-based models of stars have a number of known
problems.  The atmospheres are generally one-dimensional; three-dimensional
models remain computationally impractical for more than just a few stars.  As a
consequence of the limited atmosphere dimensionality, crude (knowingly 
incorrect) approximations for the convection and turbulence are necessary. 
Although grids of three-dimensional hydrodynamic models have been produced 
and averaged to one-dimensional approximations, these models assume local
thermodynamic equalibrium (LTE).  Properly accounting (or even approximating) 
departures from LTE is a formidable analytic and computational challenge.
Lastly, while laboratory efforts have thoroughly improved much of the faulty
atomic and molecular data, this process is unquestionably incomplete. For these
reasons there are some spectral features that are much better understood than
others.  As a consequence, physics-based methods are restricted to spectral
regions and parameter spaces that are understood marginally better, which vastly
limits their applicability and interpretability \emph{by construction}.


In detail, physics-based models do not explain all pixels of stellar 
spectroscopy at the precision with which we are currently observing.  The data
quality have outgrown the classical methods used to analyse them.  This led us 
to build \TheCannon\footnote{It is important (to us) to note that \TheCannon\ 
is named not after a weapon but instead after Annie Jump Cannon, who was the 
first to correctly order stellar spectra in temperature order \citep[and who did
so by looking at the data, and without any use of physics-based models, see][]
{Cannon_1911}.} (\citealt{tc}), a data-driven---as opposed to 
physics-based---model for stellar spectra.  \TheCannon\ is a data-driven model, 
but it differs from standard machine-learning approaches because it contains an 
explicit noise model. Given a representative set of stars with known labels of 
high-fidelity (see below), \TheCannon\ provides a generative model for stellar 
spectra based on a linear combination of the labels.  

%DWH: Cite some relevant literature.

Before we introduce the construction of the model and the tests we employed to
evaluate it, we first need to introduce the relevant terminology.  In all that 
follows, we will call stellar parameters ($\Teff$ and $\logg$) and the full set
of 15 chemical abundances collectively ``labels''.  This unifies and collapses 
the phrase ``stellar parameters and chemical abundances'' to a word, and 
connects to relevant terminology for supervised methods in the machine-learning
and statistics literatures.  Consider a spectroscopic survey that contains 
\emph{survey objects}.  Within that set of survey objects is a subset of stars 
where the data have high S/N ratios such that we can assert the labels are 
reported with high fidelity.  We will call this sample the \emph{labelled set}, 
whereas all other survey objects are categorized to the \emph{unlabelled set}.  
That is to say, although objects in the \emph{unlabelled set} may have reported 
labels (e.g., from a Survey pipeline), we are going to construct (train) 
\TheCannon\ using data in the labelled set and estimate (test) labels for those
in the unlabelled set.  In practice we specify that the \emph{training set} is
a subset of the labelled set, allowing for stars with high fidelity labels to be
excluded from training, but later used to confirm (validate) the 
predictive power of our model.  This means that \TheCannon\ can transfer labels
from high S/N training set stars to low S/N test-set stars; that is, the 
training set and the test set do not need to be statistically identical.  This 
is related to the fact that \TheCannon\ is an interpretable model; the internals
of the model are the dependencies of the spectral expectation and variance on
wavelength and physical parameters of the star. 


%DWH: Labels for data at wavelengths where there are no good labels.

%DWH: Labels that are consistent across surveys and wavelengths.

%DWH: Should we be saying things about the fact that the model is probabilistic?
%It takes as input stellar labels and gives as output a pdf for stellar spectra?

%DWH: The differences between measurement and prediction...


In the first work we did with \TheCannon, we only used a small number of labels
(three in the original work, and four or five in late work; \citealt{tc, age}). 
Here we were guided by thoughts related to density estimation: As the length of 
the label list $K$ grows, so too does the model complexity.  Sampling well a 
$K$-dimensional label space takes a training set the size of which scales 
exponentially (or worse) with $K$.  Subsequent experiments, however, did not 
bear this out. We found that we can transfer many labels from the training set 
to the test set, with training sets of thousands of stars.  The fundamental 
reason is that \TheCannon\ is \emph{not} a density estimator!  It is more like 
an \emph{interpolator}, which effectively finds stars in the training set that 
are close to the test star, and transfers labels, using the smooth polynomial 
model as a kind of regularizer.


The capacity to extend to a larger set of $K$ labels without significant
computational detriment offers tantalizing opportunities.  The most 
straightforward would be to include abundances of individual elements as labels.
However in doing so, it can be shown that a standard \emph{Cannon} model yields
coefficients (spectral derivatives; see next section) that are incompatible with 
expectations from physics: there may be an abundance label with non-zero 
contributions at all pixels, however physically we know that spectral lines of a
single element do not contribute at all wavelengths. 


For this reason alone we know that the problem is sparse.  Here we exploit this 
information to the fullest, using standard regularization methods to discover
and enforce sparsity. We consider the \emph{entire} 17-dimensional label space 
produced by the \apogee\ \aspcap\ pipeline.  We accept the \aspcap\ labels for 
a subset of the highest S/N stars, and adopt these stars and their labels as the
labelled set.  We show by validation that we can transfer these labels to much 
lower S/N stars, with reduced precision but no strong biases.  We then use the
system to label all of the stars in the \apogee\ \dr\ data set.  After
internally validating our model, we verify our abundance precision using tests
of globular and open clusters.



\section{Method}

\noindent{}We assume the following about \TheCannon\ and the \apogee\ \dr:
\begin{itemize}
\item
Stars with similar labels (parameters and abundances) have similar spectra.
\item
The expectation of the spectrum of a star is a smooth function of the values of 
the labels for that star.  Further than this, we assume that the function is so 
smooth it is reasonably approximated with a quadratic form in label space.
\item
The resolution of all \apogee\ spectra are identical, all spectra are calibrated
to the same rest wavelength grid, the noise is Gaussian and independent from 
pixel to pixel (with correctly known variances at the pixel level).  
Importantly, we are \emph{not} assuming that different stars have similar noise
variances, nor that the labelled and unlabelled sets have the same noise model.
\item
We have a training set of stars with accurate labels, where ``accurate'' is 
defined by the accuracy requirements of the output labels.  It might be more 
appropriate to say that we are assuming that the training set stars have 
\emph{consistent} labels (consistent with the assumptions of smoothness, above).
\item
The training set is representative, in the sense that the training set stars 
span the label space similarly to how any test or prediction set spans the label
space.
\item
We assume our continuum normalization procedure (described below) is consistent.
Note that we do not require \emph{true} continuum-normalization in the classical
sense because any offset (even a label-dependent residual due to a strong 
absorption line) can be captured by the model.  Instead we require that our 
normalization procedure is invariant with respect to S/N ratios.
\end{itemize}


\noindent{}Given these assumptions, the model we adopt is
\begin{eqnarray}
  y_{jn} &=& \vecv(\ell_n)\cdot\vectheta_j + e_{jn}
  \label{eq:model}\quad ,
\end{eqnarray}
where $y_{jn}$ is the data for star $n$ at wavelength pixel $j$, $\vecv(\ell_n)$
is a function that takes as input the label list $\ell_n$ of length $K$ for star
$n$ and outputs a vector of length $D>K$ of functions of those labels,
$\vectheta_j$ is a vector of length $D$ of parameters controlling the model at 
wavelength pixel $j$, and $e_{jn}$ is a noise draw or residual.  We refer to 
$\vecv(\ell_n)$ as ``the vectorizing function'', which allows for arbitrarily 
complex functions that might not be simple polynomial expansions of the label 
list $\ell_n$ (e.g., sums of sines and cosines).  Inasmuch as the model is good,
the noise values $e_{jn}$ can be taken to be drawn from a Gaussian with zero 
mean and variance $\sigma^2_{jn}+s^2_j$, where $\sigma^2_{jn}$ is the 
pipeline-reported uncertainty variance on datum $y_{jn}$ and $s^2_j$ is a 
parameter describing excess variance at wavelength pixel $j$.


Two comments about the model (\ref{eq:model}).  The first is that, because the 
$e_{jn}$ are thought of as being drawn from a probability density function (pdf),
it is a probabilistic model for the spectral data $y_{jn}$.  The second is that
the output of the function $\vecv(\ell)$ can be thought of as a row of the 
``design matrix'' that defines the possible freedom given to the spectrum 
expectation model.


In the \emph{training step}, we fix the $K$-lists of labels $\ell_n$ for all 
training set stars $n$.  We seek, at each wavelength pixel $j$, the $[D+1]$ 
parameters $\vectheta_j,s^2_j$ that optimize a penalized likelihood:
\begin{eqnarray}\label{eq:train}
  \vectheta_j,s^2_j &\leftarrow& \argmin{\vectheta,s}\left[
    \sum_{n=0}^{N-1} \frac{[y_{jn}-\vecv(\ell_n)\cdot\vectheta]^2}{\sigma^2_{jn}+s^2}
    + \sum_{n=0}^{N-1} \ln(\sigma^2_{jn}+s^2)
    + \Lambda_j\,Q(\vectheta)
    \right]
  \quad ,
\end{eqnarray}
where $\Lambda_j$ is a regularization parameter, and $Q(\vectheta)$ is a 
regularizing function that encourages parameters to take on zero values.  The 
regularizing function takes a $D$-vector as input and returns a scalar value.
We call this penalized likelihood---the argument of the argmin in 
equation~(\ref{eq:train})---the \emph{training scalar}.  We will adopt for the 
regularizing function $Q(\vectheta)$ in the training scalar a modification of L1
regularization, discussed below.  Although the training-step optimization 
problem will not in general be convex, we can make choices for $Q(\vectheta)$ 
(and we will) to make the problem such that it would be convex at any fixed 
value of $s^2$; for this reason it will tend to optimize well in most cases of 
interest.


The regularization parameter $\Lambda_j$ sets the strength of the 
regularization; as $\Lambda_j$ increases, the number of non-zero components of 
the parameter vector $\vectheta_j$ will decrease.  We give the regularization 
parameter a subscript $j$ because in general we can set it differently at every
wavelength.  This makes sense, because different wavelengths have very different
dependences on components of the label list $\ell$.  In practice the
value of $\Lambda_j$ should be set by full cross-validation, and possibly 
include restrictions based on physical arguments (e.g., a particular element 
does not have any spectral lines near this pixel, therefore the contributions
from this label must be zero).  This vastly expands the number of potential 
hyperparameters, and thus the computing expense required to determine them.  
Thus for the purpose of this work we will set a single value of $\Lambda$ (for 
all $j$ pixels) by validation.


In the \emph{test step}, we fix the parameters $\vectheta_j,s^2_j$ at all
wavelength pixels $j$.  We seek, for each test-set star $m$, the $K$-list of 
labels $\ell_m$ that optimizes the likelihood:
\begin{eqnarray}\label{eq:test}
  \ell_m &\leftarrow& \argmin{\ell}\left[
    \sum_{j=0}^{J-1} \frac{[y_{jm}-\vecv(\ell)\cdot\vectheta_j]^2}{\sigma^2_{jm}+s^2_j}
    \right]
  \quad .
\end{eqnarray}
If the vectorizing function $\vecv(\ell)$ is non-linear (as it is in our 
quadratic model), the test-step optimization is not convex.  However, the fact
that there are many pixels $j$ acting, each of which has a different functional
dependence on the labels in the label list $\ell$, tends to make the 
optimization find a good value for the label list $\ell$ in practice.  We call
this partial log likelihood---the argument of the argmin in 
equation~(\ref{eq:test})---the \emph{test scalar}.


The model freedom of \TheCannon\ is set by the vectorizing function 
$\vecv(\ell)$---which takes the $K$-element label list $\ell$ and expands it 
into a $D$-dimensional vector of components for the linear model---and the 
regularization $\Lambda_j\,Q(\vectheta)$.  Because we want the (simple, see 
below) regularization to treat the different parameters (the different 
components of $\vectheta$) in some sense ``equally'', we have to make sensible 
choices in the vectorizing function $\vecv(\ell)$.  One thing that the 
vectorizing function $\vecv(\ell)$ can do is offset the labels by some kind of
fiducial (mean, median, or other central) value, such that $\vecv(\ell)=0$ is at
a central location in the label space.  Another is to divide out a scale, 
because, for example, $\Teff$ values are in the thousands, but $\logg$ values
are of order unity.  If scale is not divided out, the (isotropic in $\vectheta$)
regularization will be much more harsh, effectively, on some parameters than 
others.  An extended version of this work might even consider different 
regularization terms for each $j$ pixel \emph{and} every label in $\ell$.


In what follows, we adopt the median value in the training set for each label 
value as the fiducial offset for that label.  We choose a dimensionless scale
factor $f$ times the label range (defined as the difference between the 97.5th
percentile and the 2.5th percentile of the training set along each label 
direction) as the scale for that label such that, for example, the $\Teff$ 
value for star $n$ is rescaled as:

\begin{eqnarray}\label{eq:label-norm}
  \hat{T}_{\mathrm{eff},n} = \frac{T_{{\rm eff},n} - \langle\Teff\rangle}{f\cdot|p_{\Teff,97.5} - p_{\Teff,2.5}|}
\end{eqnarray}

\noindent{}where $p_{\theta,k}$ is the $k$-th percentile value of $\theta$ in
the training set labels. For the regularizer $Q(\vectheta)$ we adopt a variant
of L1 regularization; we set

\begin{eqnarray}\label{eq:l1-variant}
  Q(\vectheta) &=& \sum_{d=1}^{D-1} |\theta_d|
  \quad,
\end{eqnarray}
where the sum is over the $[D-1]$ components of $\vectheta$, excluding the 
zeroth component because we don't ever expect that component to vanish.\footnote{Forgive a notational similarity here: When we subscript $\vectheta$ with $j$ we
mean ``the $D$-vector of parameters associated with wavelength pixel $j$''; when
we subscript $\theta$ with $d$ we mean ``the single parameter along coordinate
axis $d$ in the $D$-dimensional $\vectheta$ vector space.}  There is a great 
deal of theory about this kind of regularization; it is called L1 or the LASSO
(DWH CITE STUFF).  
L1 regularization encourages parameters to vanish precisely but
doesn't break convexity for convex problems.

% SPACING UP TO HERE.

%DWH: To set the amplitude $\Lambda_j$ at each wavelength pixel $j$, we
%employ a validation of the following form:....

%DWH: Scaling argument that, in general, the best value of $\Lambda_j$
%should be on the same order as the number of stars in the training
%set....

%DWH: Note that the vectorizer scales $\Delta$ play into the
%regularization, because the bigger the $\Delta$, the more penalized
%the cross (quadratic) terms become relative to the linear terms...


\section{Training, validation, and test data}

We employ the \apogee\ Data Release 12 data to demonstrate the effectiveness of
\TheCannon\ and the regularization approach in a high-dimensional label space.
We constructed our labelled set using sensible criteria for stars analysed with 
version \texttt{v603} of the \aspcap\ pipeline.  We employed several quality criteria 
to distill a robust yet representative labelled set: We first removed any stars
with problematic flags from the \aspcap\ pipeline, requiring 
\texttt{ASPCAPFLAG = 0}.  We excluded stars with S/N ratios outside the range of
200-300, and stars with a radial velocity scatter larger than 1~km~s$^{-1}$.  We
further demanded that our training set include reported abundances in all 15 
elements (C, N, O, Na, Mg, Al, Si, S, K, Ca, Ti, V, Mn, Fe, and Ni), and 
restricted the abundance range such that $2 > \mathrm{[X/Fe]} > -2$, 
$\mathrm{[Fe/H]} > -3$, and $[\alpha/\mathrm{Fe}] > -0.1$.  A visual comparison 
of the [V/H] labels with other (Fe-peak) abundance labels showed that many stars
in the labelled set had spurious measurements of [V/H].  For this reason we 
further constrained the labelled set such that $\mathrm{[V/Fe]} > -0.6$.  The 
distilled sample includes 14,141 red giant branch stars that will form our 
labelled set, with [Fe/H] labels ranging from $\mathrm{[Fe/H]} = -2.10$ to +0.30~dex. 


For the purposes of validating our model, we have randomly
assigned each star in the labelled set a uniformly-distributed integer $q$ between 0
and 9, inclusive.  We assign stars with $q > 0$ as belonging to the
training set, and those with $q = 0$ will form the validation set.  In all
experiments described in the following section, we have ensured that the same $q$ integer is assigned to each
star within the labelled set.  The training set comprises 12,681 red giant
branch stars, and the validation set includes 1,460 stars.


Here we describe the preparatory methods required before we can train the spectral model.
The \apogee\ \texttt{apStar} files contain rest-wavelength, resampled fluxes 
from individual visits for a given star, and an error array for those fluxes.
Although the \texttt{aspcapStar} files contain stacked, pseudo-continuum 
normalized spectra for a given star, we chose to only use the fluxes from the 
\texttt{apStar} data files throughout this work.  Our reasoning is as follows. 
The \aspcap\ pipeline uses a running filter window to determine the continuum.  
For this reason it is provably variant with the S/N ratio: at low S/N ratios the 
inferred continuum will be systematically shifted with respect to the same star 
observed in high S/N.  Although our labelled set only includes high S/N spectra,
our results would suffer if we employed the \aspcap\ normalization procedure for
low S/N spectra in the test set.  Thus, we opted to normalize and stack the
fluxes from individual visits provided in the \texttt{apStar} files.


Individual spectra in the \texttt{apStar} files contain associated error arrays
for each observation. However the error arrays alone do not encapsulate all
knowledge about technical, observational, or reduction issues.  Instead, every
pixel from a single observation contains a bitmask flag that documents any potential issues.  Although flagged, these issues are not reflected in the error
arrays of the individual observations.  For this reason we chose to 
construct an (adjusted) inverse variance array that encapsulates flagged pixels.
Specifically for flagged pixels we set the error array such that:


\begin{equation}
\sigma_{j,adjusted}^2 = \sigma_{j}^2 + \Delta_{j}^2
\end{equation}

\noindent{}where

\begin{equation}
\Delta_{j} = \max{\left(C_{0}|y_{j} - \widetilde{y}|,C_{1}\widetilde{y}N_{flagged}\right)} .
\end{equation}

Here $N_{flagged}$ is the number of flagged pixels in the spectrum, and (what
we call the conservatism) constants $C_0$ and $C_1$ have been chosen as 
$\{2.0,0.1\}$, which produces reasonable inverse variance values for flagged
pixels.  We chose to ignore bitmask values associated with persistence 
(specifically values 9, 10, and 11) as frequently every pixel in the blue CCD
would be flagged with these values, thereby producing unrealistically large 
uncertainties at every pixel in the blue CCD.


After updating the inverse variance arrays to account for flagged pixels, we
pseduo-continuum-normalized the individual observations. The spectra were 
normalized in three different regions, corresponding to each CCD, between
wavelength regions 15090--15822~\AA, 15823--16451~\AA, and 16452--16971~\AA.
We identified continuum pixels 
\citep[following the initial identification in][]{tc}, and for every region in each observation we fit
the continuum-pixel fluxes as a sum of sine and cosines with $L = 1400$ and $W = 3$:


\begin{equation}
y_j = \sum_{w=0}^{W} A_{2w}\sin{\left(\frac{2w\pi\lambda_{j}}{L}\right)} + A_{(2w+1)}\cos{\left(\frac{2w\pi\lambda_{j}}{L}\right)}.
\end{equation}

Using a sum of sines and cosines for pseudo-continuum normalization has a
number of advantages over alternative approaches used for continuum normalization.
Firstly, it is a linear operation on the fluxes and therefore cheap.
More importantly, the use of sine and cosine functions implies that edge behaviour of the
continuum function will be bounded (more bounded than a polynomial
function), and are therefore less susceptible to issues with overfitting 
where the edge of the continuum function demonstrates large wiggles.


It is important to note that this approach is \emph{pseudo}-continuum normalization.
While our choice of continuum pixels is well-informed, albeit arbitrary, this
procedure does not require that the chosen pixels to indeed be true continuum
pixels.  The `continuum' pixels could be randomly selected, and as long as the
same pixels were used to normalize all stars, our approach would work equally
well: all spectra would be `normalized' in the same way, and the residual flux
behaviour from a `true'-normalized spectra would be captured by the spectral
model.  We have only attempted to select well-informed 
continuum pixels in order to maximize model interpretability.  That is to say,
by using reasonable continuum pixels we can be sure that any spectral model
derivatives can be interpreted as being astrophysically motivated, and less likely to
simply be capturing residual continuum effects.


For all of the aforementioned reasons (invariance with respect to S/N, bounded
functions, linear operations, and repeatability between spectra), we emphasize
that if traditional continuum methods are employed, \TheCannon\ will (likely) 
give \emph{very bad results}.  Adopting a robust continuum normalization 
procedure is paramount.  After normalizing all individual spectra in the
\texttt{apStar} files, we re-stacked the spectra using the (adjusted) inverse variance of
each pixel as weights.  Therefore for all stars observed by
\apogee, we have normalized individual and combined spectra, allowing for 
a self-consistent examination of label determination at low S/N (see later section).



% ARC: Should we move this paragraph somewhere else?
The line spread function of \apogee\ spectra is wavelength- and fibre-dependent.
Given this information, our current implementation of \tc\ is knowingly 
sub-optimal: we assume nothing about differing line spread functions between 
stars in the labelled or unlabelled set. Similarly, we make no effort to 
accommodate fast-rotating stars, where the effect on the spectrum is 
approximately represented by convolution with a Gaussian kernel (in the same way
a lower resolution would be).  A useful extension of this work would be to 
simultaneously solve for any additional rotation at test time.



%To demonstrate the predictive power and accuracy of the method, we
%will (below) perform a conservative cross-validation using cyclic
%training, validation, and test subsets of the labeled data:
%To each star in the input set of labeled stars (the superset of any
%possible training set we might use), we assign an integer $0\leq
%q<10$, with uniform probability across all 10 possible values of $q$.
%This permits a very conservative 10-fold cross-validation, in which we
%choose $8/10$ of the labeled data as the training set, a disjoint
%$1/10$ as a validation set for choosing the hyper-parameters
%$\Lambda_j$, and a mutually disjoint $1/10$ as a test set for
%prediction.
%This train--validate--test framework is very conservative, because there
%is no way, for each choice of train, validate, and test sets from the
%labeled data, for the test set to influence the choice of
%hyper-parameters, or indeed for any information to flow from the test
%set into the training.

%When \TheCannon\ is used to label the full unlabeled set, we train on the entire
%unlabled data set (the largest possible training set) and
%use values for the hyper-parameters $\Lambda_j$ derived from the
%10-fold cross-validation experiment, as we will describe below.

% ARC: Figure: HRD (coloured by [Fe/H]) of the labelled set.

\section{Experiments}
\label{sec:experiments}

\subsection{Hyper-parameter selection and validation}
\label{sec:hyper-parameter-validation}
In the simplest case we have two hyper-parameters that need to be 
determined: $\Lambda$ and $f$.  The strength of the regularization (at all 
pixels) is set by $\Lambda$, and the scale factor $f$ controls the scaling on 
individual labels.  In practice large $\Lambda$ values encourage zero 
coefficients (unless it is well-fit by the data) and thus enforces sparsity.  
High scale factor $f$ values act to penalize cross-terms more than linear labels. 
Because these hyper-parameters act in conjunction with each other, we need to
explore many combinations of $\Lambda$ and $f$.


Here we use a single training set and perform a grid search to evaluate models
with different values of $\Lambda$ and $f$.  Specifically we vary $\Lambda$ in
logarithmic steps from $10^0$ to $10^5$, and linearly increase the scale factor
$f$ from 0.5 to 50.  For each combination of $\Lambda$ and $f$ we trained the full
17-label model and fitted labels for all (individual and stacked) spectra in
the validation set.


We must chose heuristic(s) to select the optimal regularization $\Lambda$ and
scale factor $f$.  Many metrics are available, including: the predictive power
in spectral fluxes (e.g., the $\chi^2$ value for the validation set), the 
label precision at low S/N, or opting to maximize the model sparsity.  In practice these metrics must be
balanced if we want a sparse, interpretable model that yields precise labels
even at low S/N values.


We define the model sparsity as the percent of zero-value spectral derivatives $\vectheta$.
The sparsity could be calculated across just the linear coefficients, only
the cross-term coefficients, or some combination thereof.  Note that we never
include the baseline spectrum coefficients $\theta_0$ when calculating 
sparsity metrics because this parameter is not regularized (see equation 
\ref{eq:l1-variant}).  In Figure \ref{fig:sparsity} we show three different
sparsity metrics for many permutations of $\Lambda$ and $f$.  Specifically we
show the sparsity of the linear model coefficients $\vectheta_{1...17}$, the 
cross-term coefficients $\vectheta_{18...170}$, and the combination of linear
and cross-term coefficients.  It is clear that the total model sparsity does
not change significantly (regardless of $f$) until $\Lambda \gtrsim 10^3$.  
In this regime
the linear and second-order coefficient sparsity metrics exhibit very
different responses.  The cross-term sparsity increases faster than the
linear terms, with a clear dependence on the scale factor $f$.  Note also
that there is an `optimal' scale factor $f \approx 20$ which produces the 
sparsest model for a given regularization factor.


While sparser models are preferred, our ultimate goal is to have an
interpretable  model that predicts spectral fluxes and returns precise
stellar labels.  In Figures \ref{fig:gridsearch-all} and XXXXX we show the
median absolute difference (MAD) between our estimated label value at low 
S/N values (from individual spectra) and the corresponding high S/N (stacked)
spectrum.  This is an internally-consistent check: we will validate our high-
(and low-) S/N label determination against the \aspcap\ values in the next
section.  It is clear from these figures that the optimal combination of 
$(\Lambda, f)$ is dependent on the label of interest.  However for all 
labels it is clear that a combination of decreasing $f$ with increasing
$\Lambda$ will yield good results.  


Our grid search has revealed that a regularization factor of $\Lambda = 10^3$ and
a scale factor $f = 2$ yields a relatively sparse (interpretable) model that
accurately predicts spectral fluxes and provides precise stellar labels at low S/N. 
 We adopt these values for the remainder of this work.


\subsection{Label recovery as a function of signal-to-noise}
\label{sec:label-recovery-snr}

Recall that the \emph{labelled set} includes both the \emph{training set} and
a disjoint \emph{validation set}.  Here we seek to understand the model performance
as a function of S/N after setting the hyper-parameters $\Lambda$ and $f$.
The stars in the validation set include stacked
spectra as well as the individual normalized observations.  We apply our model to
all stacked and individual spectra and recover labels for every observation. 
This permits us to understand how well we can recover labels (to some precision)
for a given S/N.  Crucially, this experiment provides a very model-independent
metric for performance.  Analysis pipelines that have larger uncertainties
at a given S/N are demonstrably imprecise (accuracy is not tested here, unless,
perhaps, we compare to the ASPCAP labels from the validation set).

% Discuss the figure.

% At S/N X we get precision of what in what.

\subsection{Duplicate APOGEE/ASPCAP IDs}
\label{sec:duplicate-ids}

\subsection{Label errors \& covariances}
\label{sec:errors}



\section{Results}
\label{sec:results}

ARC: high s/n values for low and high S/N

ARC: Globular clusters

ARC: Open clusters




DWh: What can we say about chemical abundance precision and accuracy?

DWH: What can we say about chemical abundance space?

DWH: What can we say about the promise of chemical tagging?

\section{Discussion}
\label{sec:discussion}

% Model Interpretability?

% Line identification


\TheCannon\ is a prediction system: It is designed to predict the labels for 
stars.  It has a fundamental assumption that the training set is statistically
identical to the prediction/test set.  If there are covariances in the label 
space---that is, if stars in the training set are high in label $X$ whenever
they are also high in label $Y$---then there will be opportunities for 
\TheCannon\ to get information about $Y$ from considering data aspects that 
depend on $X$ (which, according to the training set, is reliably connected to 
$Y$).  For this reason, we expect that, given a finite training set, there will
be situations in which lines from one element are used to predict the abundance
of another.  A good example is shown in \figurename~\ref{fig:whatever}, where we
see WHATEVER.

DWH: Yada Yada...


ARC: The fact that we do so badly for [V/H], yet [V/H] is empirically covariant with [Fe/H] in the same way that other Fe-peak abundance labels are, tells us that there is a fundamental limit to how much empirically covariant information is entering into our results!


DWH: No accounting for variable spectroscopic resolution, nor
microturbulence nor rotation.

DWH:  Once again, the differences between measurement and prediction...

DWH:  Why is this going to change the World nonetheless?

DWH: All of the code for this project is available with documentation
at \url{http://thecannon.io/}.

\acknowledgements
% Thanks...
The authors warmly thank Daniel Foreman-Mackey for valuable discussions.
This project was funded in part by
  the European Research Council under the European Union's Seventh Framework 
  Programme (FP~7) \acronym{ERC} Grant Agreement 320360,
  the \acronym{NSF} (grants \acronym{IIS-1124794}, \acronym{AST-1517237}),
  \acronym{NASA} (grant \acronym{NNX12AI50G}), and 
  the Moore-Sloan Data Science Environment at \acronym{NYU}.
This research made use of 
  the \acronym{NASA} \project{Astrophysics Data System},
  \texttt{TOPCAT} \citep{Taylor_2005}, and 
  Astropy, a community-developed core Python package for Astronomy \citep{astropy}.

% ARC: GitHub? Travis CI?

This project made use of \sdssiii\ data.
Funding for \sdssiii\ has been provided by the Alfred P. Sloan
Foundation, the Participating Institutions, the National Science
Foundation, and the \acronym{U.S.} Department of Energy Office of Science. The
\sdssiii\ web site is http://www.sdss3.org/.

\sdssiii\ is managed by the Astrophysical Research Consortium for the
Participating Institutions of the \sdssiii\ Collaboration including the
University of Arizona, the Brazilian Participation Group, Brookhaven
National Laboratory, Carnegie Mellon University, University of
Florida, the French Participation Group, the German Participation
Group, Harvard University, the Instituto de Astrofisica de Canarias,
the Michigan State/Notre Dame/\acronym{JINA} Participation Group, Johns Hopkins
University, Lawrence Berkeley National Laboratory, Max Planck
Institute for Astrophysics, Max Planck Institute for Extraterrestrial
Physics, New Mexico State University, New York University, Ohio State
University, Pennsylvania State University, University of Portsmouth,
Princeton University, the Spanish Participation Group, University of
Tokyo, University of Utah, Vanderbilt University, University of
Virginia, University of Washington, and Yale University.

All of the code used to reproduce this research is available in the open-source
repository at \url{https://github.com/andycasey/AnniesLasso}.



\begin{thebibliography}{dummy}\raggedright
\bibitem[Astropy Collaboration et 
al.(2013)]{astropy} Astropy Collaboration, Robitaille, T.~P., Tollerud, E.~J., et al.\ 2013, Astronomy \& Astrophysics, 558, AA33 
\bibitem[Ness et al.(2015a)]{tc} Ness, M., Hogg, D.~W., 
Rix, H.-W., Ho, A.~Y.~Q., \& Zasowski, G.\ 2015, \apj, 808, 16
\bibitem[Ness et al.(2015b)]{age} Ness, M., Hogg, D.~W., 
Rix, H., et al.\ 2015, arXiv:1511.08204 
\bibitem[Taylor(2005)]{Taylor_2005} Taylor, M.~B.\ 2005, Astronomical Data Analysis Software and Systems XIV, 347, 29 
\end{thebibliography}

\clearpage

\begin{figure}[p]
\caption{Choose two wavelengths (one continuum and one interesting)
  and plot some scatter plots of flux vs various parameters, and also
  cross-validation results for the
  hyper-parameters.\label{fig:onewavelength}}
\end{figure}

\begin{figure}[p]
\caption{Something about hyper-parameters and scatter as a function of
  wavelength.\label{fig:hyperpars}}
\end{figure}

\begin{figure}[p]
\caption{Something about first derivatives of the spectral expectation
  with respect to labels as a function of
  wavelength.\label{fig:derivatives}}
\end{figure}

\begin{figure}[p]
\caption{Show a few sample spectra and demonstrate that the quality of
  the model prediction is extremely good; better than
  \aspcap.\label{fig:correctness}}
\end{figure}

\begin{figure}[p]
\caption{Something showing that our results on the full unlabeled set
  make sense and seem correct!\label{fig:fulltest}}
\end{figure}

\begin{figure}[p]
\caption{Choose two elements that are interesting, and pull apart the
  differences between what we have and what \aspcap\ has for those
  elements.  We are better!\label{fig:elements}}
\end{figure}

\begin{figure}[p]
\caption{Something about the quality of results in the
  cross-validation as a function of the signal-to-noise of what we
  give in the test subset.\label{fig:snr}}
\end{figure}


\begin{figure}[p]
\includegraphics[width=\textwidth]{sparse-first-order-coefficients.pdf}
\caption{Normalized (to the maximum derivative value at any $\theta$) first-order derivatives for [Al/H], [S/H], and [K/H] from a regularized Cannon model with $\Lambda = 10^{3.5}$ and $f = 20$. For the sake of clarity, we have only shown $\theta$ values that exceed the shaded region. The top markings for K and Al correspond to the lines used by Smith et al. The black markers indicate 'Missing' or 'Unknown' lines in the APOGEE spectra according to Shetrone et al. Here we identify these elements from our model derivatives.  \label{fig:inferring-lines}}
\end{figure}

\begin{figure}[p]
\includegraphics[width=\textwidth]{sparse-first-order-coefficients-zoom.pdf}
\caption{Zoom in of previous figure, showing the theta values for all elements. Al and S are highlighted and colored as per Figure \ref{fig:inferring-lines}.\label{fig:inferring-lines2}}
\end{figure}


\begin{figure}[p]
\includegraphics[width=\textwidth]{sparsity.pdf}
\caption{The fraction of non-zero coefficients (sparsity) of 17-label models with different scale factors $f$ and regularization terms $\Lambda$.  The sparsity of the first-order coefficients are shown in the left panel, the second-order coefficients (e.g., $\Teff^2$, $\Teff\cdot\logg$, or $[\rm{Al}/\rm{H}]\cdot[\rm{Mn}/\rm{H}]$) in the middle panel, and total sparsity (excluding the baseline spectrum coefficient $\theta_0$) in the right-hand panel.\label{fig:sparsity}}
\end{figure}


\end{document}
