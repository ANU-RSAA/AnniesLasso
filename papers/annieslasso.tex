%% This file is part of the Annie's Lasso project.
%% Copyright 2015 the authors.  All rights reserved.

% To-Do
% -----
% - make full outline
% - get to zeroth draft
% - audit for usage of vector and list
% - search for all occurrences of DWH, AC, or MKN in the text and fix them.

% Style Notes
% -----------
% - Use \acronym{NASA} for acronyms like NASA.
% - The label list \ell is a *list*; the vectorization v(ell) is a *vector*.

\documentclass[12pt,preprint]{aastex}
\usepackage{amsmath,amssymb}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\TheCannon}{\project{The~Cannon}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\sdss}{\project{\acronym{SDSS-IV}}}
\newcommand{\apogee}{\project{\acronym{APOGEE2}}}
\newcommand{\aspcap}{\project{\acronym{ASPCAP}}}
\newcommand{\dr}{\acronym{DRXX}}
\newcommand{\logg}{\log g}
\newcommand{\mh}{\mathrm{[M/H]}}
\newcommand{\Teff}{T_{\mathrm{eff}}}
\newcommand{\Dvector}[1]{\boldsymbol{#1}}
\newcommand{\vectheta}{\Dvector{\theta}}
\newcommand{\vecv}{\Dvector{v}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}\,}

\begin{document}

\title{\textsl{The Cannon 2:} A data-driven model of stellar spectra \\
       for detailed chemical abundance analyses}
\author{AC, DWH, MKN, others}

\begin{abstract}
% context
In previous work, we have shown that it is possible to train a generative
probabilistic model for stellar spectra using a training set of stars, each with known
parameters ($\Teff$, $\logg$, $\mh$), and then use that model to infer labels for
unlabeled stars, even stars with lower signal-to-noise observations.
% aims
Here we ask whether this is possible when the dimensionality of the chemical
abundance space is large (15 abundances: Mg, Fe, etc, etc, Al)
and the model is non-linear in its response to abundance and parameter changes.
% method
We adopt ideas from compressed sensing to limit overall model complexity (number
of non-zero parameters) while retaining model freedom.
The training set is a set of YYY red-giant stars with high signal-to-noise
spectroscopic observations and stellar parameters and abundances taken from the
\apogee\ Survey.
% results
We find that we can successfully train and use a model with 17 stellar labels.
Cross-validation shows that the model does a good job of inferring all 17 labels
(with the exception of XXX), even when we degrade the signal-to-noise of the
validation set by discarding some of the spectroscopic observing time.
The model dependencies make sense; the derivatives of the spectral mean model
with respect to abundances correlate well with known atomic lines.
We deliver open-source code and also stellar parameters and 15 abundances for a
set of ZZZ stars.
\end{abstract}

\section{Introduction}


The detailed surface abundances of a star reflect its formation environment
and the supernova(e) that preceded it.  Minute perturbations in the properties
of either can produce subtly different chemical abundance patterns on successive
generations of stars.  This chemical abundance signature largely remains 
constant in the stellar photosphere throughout a star's lifetime, thereby 
providing a fossilised record of the gas composition during formation and the 
imprinted signature of stars that preceded it.  The variation between these
signatures is small, implying that it is difficult to distinguish subtle 
chemical patterns from different formation sites.  However if they were 
distinguishable, a sufficiently large collection of stellar abundances would 
unambiguously unravel the complete chemical evolution of the Milky Way.


There are some exceptions to this line of reasoning; the abundances of some
elements can (and do) change throughout a star's lifetime.  These are usually
subtle signatures that are reflective of intriguing astrophysical phenomena:
atomic diffusion, evolutionary mixing, the presence (or accretion) of 
exoplanets, material transfer from a stellar binary companion, and countless
other scenarios.  In the same way that precise abundances are required to 
identify unique chemical formation signatures, precise abundances are necessary
to distinguish minute evolutionary or environment differences between subsets of
stars.  In these scenarios we argue most astrophysicists care less about
accuracy; it is the abundance precision that imparts discovery and understanding
of these astrophysical phenomena.  Indeed, in most studies of stellar
astrophysics \textit{an increase in precision yields far more astrophysical 
insight than a comparable increase in accuracy}.


For these reasons a comprehensive catalog of \textit{precise} detailed chemical
abundances would revolutionise our understanding of planetary, stellar, and 
galactic astrophysics.  This goal has only recently become feasible given the 
increasing volume and quality of stellar spectra obtained in the last decade.  
Specifically, large surveys are obtaining high-resolution 
($\mathcal{R} \gtrsim 20,000$), high signal-to-noise (S/N) ratio spectra for
$\sim10^{5}$---$10^{6}$ stars across all components of the Galaxy.  This is a
sharp relative increase in data volume, which has mandated the automatic
analysis of stellar spectra, and persuaded dozens of groups to produce bespoke
pipelines.



Most automated pipelines have grown from classical, manual methods.  That is to
say, there has been relatively little work on unconventional methods to analyse
spectra.  Spectroscopists have hard-coded their intuition (or subjectivity),
with arbitrary heuristics to decide issues of wavelength masks, convergence
criteria, etc.  Most of these decisions are based on optimizing the resultant
accuracy for a few stars with extremely high quality data (e.g., high S/N).  As
a result, these heuristics are frequently incomparable for more data with more
modest (and representative) S/N ratios.  Therefore, for most stars in the sample,
it can be trivially shown from repeat or blind experiments that that individual
pipelines yield demonstrably imprecise abundances for S/N ratios that are 
most typical in the data.  Moreover, results from individual pipelines vary 
significantly with respect to each other, with abundance differences an order
of magnitude larger than the aforementioned astrophysical signatures.
%Thus, while there has been substantial effort in automating classical analysis
%techniques, they are generally imprecise at modest S/N ratios, and yield
%different and inaccurate results when compared against each other.

Considerable efforts have also focussed on improving the accuracy of physical
models.  Indeed it is impossible to measure physical properties of stars (or 
their chemical abundances) accurately without accurate physical models of 
stellar spectra.  However, physics-based models of stars have a number of known
problems.  The atmospheres are generally one-dimensional; three-dimensional
models remain computationally impractical for more than just a few stars.  As a
consequence of the limited atmosphere dimensionality, crude (knowningly 
incorrect) approximations for the convection and turbulence become necessary. 
Although grids of three-dimensional hydrodynamic models have been produced 
and averaged to one-dimensional approximations, these models still assume local
thermodynamic equalibrium (LTE).  Properly accounting (or even approximating) 
departures from LTE is a formidable analytic and computational challenge.
Lastly, while laboratory efforts have thoroughly improved much of the faulty
atomic and molecular data, this process is unquestionably incomplete.  For these
reasons there are some spectral features that are much better understood than
others.  As a consequence, physics-based methods are restricted to spectral
regions or parameter space that are understood marginally better, which vastly
limits their applicability and interpretability by construction.


%Even the opacities employed by modern radiative transfer codes are polynomial approximations that are decades old.

In detail, physics-based models do not explain all pixels of stellar 
spectroscopy at the precision with which we are currently observing.  The data
quality have outgrown the methods currently used to analyse them.  This led us 
to build \TheCannon\ (\citealt{tc}), a data-driven---as opposed to physics-based
---model for stellar spectra. 





%It is impossible to make accurate measurements of the physical
%properties of stars, or their chemical abundances, without accurate
%physical models of stellar spectra.
%However, the physics-based models for stars have a number of known
%problems:
%They are generally one-dimensional rather than three-dimensional, and
%either don't include or else have only effective models for some of
%the relevant physical processes.
%The input atomic and molecular physics information is faulty and
%incomplete.
%There are some wavelengths and lines that are much better understood
%than others.
%In detail, the models do not explain all pixels of stellar
%spectroscopy at the precision with which we are currently observing,
%which is signal-to-noise ratios in the hundreds for thousands or tens
%of thousands of wavelength pixels per star.


DWH: Summarize what it does and what it can be used for.
DWH: Mention some users and projects, past, current, and future.
DWH: Cite some relevant literature.
It is important (to us) to note that \TheCannon\ is named not after a
weapon but instead after Annie Jump Cannon, who was the first to
correctly order stellar spectra in temperature order (and who did so
by looking at the data, and without any use of physics-based models).

In all that follows, we will call stellar parameters ($\Teff$ and
$\logg$) and the full set of 15 chemical abundances
collectively ``labels''.
This unifies and collapses the ``stellar parameters and chemical
abundances'' phrase to a word, and also connects to the terminology
for supervised methods in the machine-learning and statistics
literatures.

DWH: Define the words ``train'', ``validate'', and ``test''.  And maybe ``predict'' too?

DWH: Labels for data at wavelengths where there are no good labels.

DWH: Labels that are consistent across surveys and wavelengths.

\TheCannon\ is a data-driven model, but it differs from standard machine-learning
approaches because it contains an explicit noise model.
This means that it can transfer labels from high signal-to-noise training-set
stars to low signal-to-noise test-set stars; that is, the training set and 
the test set do not need to be statistically identical.
This is related to the fact that \TheCannon\ is an interpretable model;
the internals of the model are the dependencies of the spectral expectation
and variance on wavelength and physical parameters of the star.

DWH: Should we be saying things about the fact that the model is probabilistic?
It takes as input stellar labels and gives as output a pdf for stellar spectra?

DWH: In the first work we did with \TheCannon, we only used a small
number of labels (three in the original work, and four or five in late
work; \citealt{tc, age}).
Here we were guided by thoughts related to density estimation:
Sampling well a $K$-dimensional label space takes a training set the
size of which scales exponentially (or worse) with $K$.
Subsequent experiments, however, did not bear this out:
We found that we can transfer many labels from the training set to
the test set, with training sets of thousands of stars.
The fundamental reason is that \TheCannon\ is \emph{not} a density estimator!
It is more like an \emph{interpolator}, which effectively finds stars in
the training set that are close to the test star, and transfers labels,
using the smooth polynomial model as a kind of regularizer.

DWH: Here we exploit this to the fullest.
We consider the \emph{entire} 17-dimensional label space produced by
the \apogee\ \aspcap\ pipeline.
We accept the \aspcap\ labels for the highest
signal-to-noise stars, and adopt these stars and their labels as the
training set.
We show, by very conservative cross-validation, that we can transfer these
labels to much lower signal-to-noise stars, with reduced precision but no
strong biases.
We then use the system to label all of the stars in the \apogee\ \dr\ data set.

DWH: The differences between measurement and prediction...

DWH: Etc!

\section{Method}

DWH: What are we going to assume?

The model is
\begin{eqnarray}
  y_{jn} &=& \vecv(\ell_n)\cdot\vectheta_j + e_{jn}
  \label{eq:model}\quad ,
\end{eqnarray}
where $y_{jn}$ is the data for star $n$ at wavelength pixel $j$,
$\vecv(\ell_n)$ is a function that takes as input
the label list $\ell_n$ of length $K$ for star $n$
and outputs a vector of length $D>K$ of functions of those labels,
$\vectheta_j$ is a vector of length $D$ of parameters controlling the model at wavelength pixel $j$,
and $e_{jn}$ is a noise draw or residual.
Inasmuch as the model is good, the noise values $e_{jn}$ can be taken to be
drawn from a Gaussian with zero mean and variance $\sigma^2_{jn}+s^2_j$,
where $\sigma^2_{jn}$ is the pipeline-reported uncertainty variance on datum
$y_{jn}$ and $s^2_j$ is a parameter describing excess variance at wavelength pixel $j$.

Two comments about the model (\ref{eq:model}).
The first is that, because the $e_{jn}$ are thought of as being drawn from a 
probability density function (pdf), it is a probabilistic model for the spectral
data $y_{jn}$.
The second is that the output of the function $\vecv(\ell)$ can be thought
of as a row of the ``design matrix'' that defines the possible freedom
given to the spectrum expectation model.

In the \emph{training step}, we fix the $K$-lists of labels $\ell_n$
for all training-set stars $n$.
We seek, at each wavelength pixel $j$, the $[D+1]$ parameters
$\vectheta_j,s^2_j$ that optimize a penalized likelihood:
\begin{eqnarray}\label{eq:train}
  \vectheta_j,s^2_j &\leftarrow& \argmin{\vectheta,s^2}\left[
    \sum_{n=0}^{N-1} \frac{[y_{jn}-\vecv(\ell_n)\cdot\vectheta]^2}{\sigma^2_{jn}+s^2}
    + \sum_{n=0}^{N-1} \ln(\sigma^2_{jn}+s^2)
    + \Lambda_j\,Q(\vectheta)
    \right]
  \quad ,
\end{eqnarray}
where $\Lambda_j$ is a regularization parameter, and $Q(\vectheta)$ is a 
regularizing function that encourages parameters to take on zero values.
The regularizing function takes a $D$-vector as input and returns a
scalar value.
We call this penalized likelihood---the argument of the
argmin in equation~(\ref{eq:train})---the \emph{training scalar}.
We will adopt for the regularizing function $Q(\vectheta)$ in the training scalar a
modification of L1 regularization, discussed below.
Although the training-step optimization problem will not in general be
convex, we can make choices for $Q(\vectheta)$ (and we will) to make the
problem such that it would be convex at any fixed value of $s^2$; for
this reason it will tend to optimize well in most cases of interest.

The regularization parameter $\Lambda_j$ sets the strength of the
regularization; as $\Lambda_j$ increases, the number of non-zero
components of the parameter vector $\vectheta_j$ will decrease.
We give the regularization parameter a subscript $j$ because in
general we can set it differently at every wavelength.
This makes sense, because different wavelengths have very different
dependences on different components of the label list $\ell$.
In what follows, we will set the value of the $\Lambda_j$ by
validation or cross-validation or possibly based on physical arguments
about what matters!

In the \emph{test step}, we fix the parameters $\vectheta_j,s^2_j$ at all
wavelength pixels $j$.
We seek, for each test-set star $m$, the $K$-list of labels $\ell_m$
that optimizes the likelihood:
\begin{eqnarray}\label{eq:test}
  \ell_m &\leftarrow& \argmin{\ell}\left[
    \sum_{j=0}^{J-1} \frac{[y_{jm}-\vecv(\ell)\cdot\vectheta_j]^2}{\sigma^2_{jm}+s^2_j}
    \right]
  \quad .
\end{eqnarray}
Because the vectorizing function $\vecv(\ell)$ is non-linear, the
test-step optimization is not convex.
However, the fact that there are many pixels $j$ acting, each of which
has a different functional dependence on the labels in the label list
$\ell$, tends to make the optimization (in practice) find a good value
for the label list $\ell$.
We call this partial log likelihood---the argument of the
argmin in equation~(\ref{eq:test})---the \emph{test scalar}.

The model freedom of \TheCannon\ is set by the vectorizing function
$\vecv(\ell)$---which takes the $K$-element label list $\ell$ and expands
it into a $D$-dimensional vector of components for the linear
model---and the regularization $\Lambda_j\,Q(\vectheta)$.
Because we want the (simple, see below) regularization to treat the
different parameters (the different components of $\vectheta$) in some
sense ``equally'', we have to make sensible choices in the vectorizing
function $\vecv(\ell)$.
One thing that the vectorizing function $\vecv(\ell)$ can do is offset the
labels by some kind of fiducial (mean, median, or other central)
value, such that $\vecv(\ell)=0$ is at a central location in the label
space.
Another is divide out a scale, because, for example, $\Teff$ values
are in the thousands, but $\logg$ values are of order unity.
If scale is not divided out, the (isotropic in $\vectheta$)
regularization will be much more harsh, effectively, on some
parameters than others.

DWH: In detail we construct the vectorizing function $\vecv(\ell)$ as follows...

For the regularizer $Q(\vectheta)$ we adopt a variant of L1
regularization; we set
\begin{eqnarray}
  Q(\vectheta) &=& \sum_{d=1}^{D-1} |\theta_d|
  \quad,
\end{eqnarray}
where the sum is over the $[D-1]$ components of $\vectheta$, excluding
the zeroth component (because we don't ever expect that component to
vanish).
(Forgive a notational similarity here: When we subscript $\vectheta$ with
$j$ we mean ``the $D$-vector of parameters associated with wavelength
pixel $j$''; when we subscript $\theta$ with $d$ we mean ``the single
parameter along coordinate axis $d$ in the $D$-dimensional $\vectheta$
vector space.)
There is a great deal of theory about this kind of regularization; it
is called L1 or the lasso (DWH CITE STUFF).
L1 regularization encourages parameters to vanish precisely but
doesn't break convexity for convex problems.

DWH: To set the amplitude $\Lambda_j$ at each wavelength pixel $j$, we
employ cross-validation...

\section{Training, validation, and test data}


We employ the \apogee\ Data Release 12 to demonstrate the effectiveness of our 
regularization approach.   We constructed our training set using a subset of 
high-quality labels provided in version \texttt{v603} from the \aspcap\ 
pipeline, and used fluxes from the individual \texttt{aspcapStar} data files.
The \texttt{aspcapStar} files contain (approximately) continuum-normalized 
spectra, often stacked from multiple visits, and an error array for those 
fluxes.  All spectra have been placed at rest-frame (vacuum wavelengths), and
resampled onto a common log-wavelength scale. In total, there are NNNNN science
objects.

We employed several quality criteria to distill a robust yet representative 
training set: We removed any stars with problematic flags from the \aspcap\
pipeline, requiring \texttt{ASPCAPFLAG = 0}.  We excluded stars with 
signal-to-noise ratios outside the range of 200-300, and stars with a radial 
velocity scatter larger than 1~km~s$^{-1}$.  We further demanded that
our training set include reported abundances in all 15 elements (C, N, O, Na, 
Mg, Al, Si, S, K, Ca, Ti, V, Mn, Fe, and Ni), and restricted the abundance range
such that $2 > \mathrm{[X/Fe]} > -2$, $\mathrm{[Fe/H]} > -3$, and 
$[\alpha/\mathrm{Fe}] > -0.1$.  A visual comparison of the [V/H] labels
with other (Fe-peak) labels showed that many stars in the training set had
spurious measurements of [V/H]. For this reason we further constrained the
training sample such that $\mathrm{[V/Fe]} > -0.6$.  The distilled training
set includes 14,141 red giant branch stars, with [Fe/H] labels ranging from
$\mathrm{[Fe/H]} = [-2.10, 0.30]$. 






MKN: What is different about our continuum normalization from everyone else's?

DWH: Define the various possible sets of ``labeled'' stars and the set of ``unlabeled''
stars, for our various purposes.

DWH: Any training set must be a subset of the
labeled stars.
We are going to use different subsets for different trainings of \TheCannon.
ARC: Not anymore we ain't.

To demonstrate the predictive power and accuracy of the method, we
will (below) perform a conservative cross-validation using cyclic
training, validation, and test subsets of the labeled data:
To each star in the input set of labeled stars (the superset of any
possible training set we might use), we assign an integer $0\leq
q<10$, with uniform probability across all 10 possible values of $q$.
This permits a very conservative 10-fold cross-validation, in which we
choose $8/10$ of the labeled data as the training set, a disjoint
$1/10$ as a validation set for choosing the hyper-parameters
$\Lambda_j$, and a mutually disjoint $1/10$ as a test set for
prediction.
This train--validate--test framework is very conservative, because there
is no way, for each choice of train, validate, and test sets from the
labeled data, for the test set to influence the choice of
hyper-parameters, or indeed for any information to flow from the test
set into the training.

When \TheCannon\ is used to label the full unlabeled set, we train on
the entire unlabled data set (the largest possible training set) and
use values for the hyper-parameters $\Lambda_j$ derived from the
10-fold cross-validation experiment, as we will describe below.

\section{Experiments}

\paragraph{Validation for hyper-parameter selection:}
Hello World!

\paragraph{Three-label case:}
Hello World!

\paragraph{Seventeen-label case:}
Hello World!

\paragraph{Label recovery as a function of signal-to-noise:}
Hello World!

\section{Results}

DWH: Do we need this section?

DWh: What can we say about chemical abundance precision and accuracy?

DWH: What can we say about chemical abundance space?

DWH: What can we say about the promise of chemical tagging?

\section{Discussion}

DWh:  Hello World!  What were our assumptions and how would things have
changed if we had changed or relaxed them?

DWH:  What is bad or limited about what we have done?

DWH: No accounting for variable spectroscopic resolution, nor
microturbulence nor rotation.

DWH:  Once again, the differences between measurement and prediction...

DWH:  Why is this going to change the World nonetheless?

DWH: All of the code for this project is available with documentation
at \url{http://thecannon.io/}.

\acknowledgements
DWH: Dan Foreman-Mackey (UW) and Hans-Walter Rix (MPIA) and other people...
grants...
\sdss...

\begin{thebibliography}{dummy}\raggedright
\bibitem[Ness et al.(2015a)]{tc} Ness, M., Hogg, D.~W., 
Rix, H.-W., Ho, A.~Y.~Q., \& Zasowski, G.\ 2015, \apj, 808, 16
\bibitem[Ness et al.(2015b)]{age} Ness, M., Hogg, D.~W., 
Rix, H., et al.\ 2015, arXiv:1511.08204 
\end{thebibliography}

\clearpage

\begin{figure}[p]
\caption{Choose two wavelengths (one continuum and one interesting)
  and plot some scatter plots of flux vs various parameters, and also
  cross-validation results for the
  hyper-parameters.\label{fig:onewavelength}}
\end{figure}

\begin{figure}[p]
\caption{Something about hyper-parameters and scatter as a function of
  wavelength.\label{fig:hyperpars}}
\end{figure}

\begin{figure}[p]
\caption{Something about first derivatives of the spectral expectation
  with respect to labels as a function of
  wavelength.\label{fig:derivatives}}
\end{figure}

\begin{figure}[p]
\caption{Show a few sample spectra and demonstrate that the quality of
  the model prediction is extremely good; better than
  \aspcap.\label{fig:correctness}}
\end{figure}

\begin{figure}[p]
\caption{Something showing that our results on the full unlabeled set
  make sense and seem correct!\label{fig:fulltest}}
\end{figure}

\begin{figure}[p]
\caption{Choose two elements that are interesting, and pull apart the
  differences between what we have and what \aspcap\ has for those
  elements.  We are better!\label{fig:elements}}
\end{figure}

\begin{figure}[p]
\caption{Something about the quality of results in the
  cross-validation as a function of the signal-to-noise of what we
  give in the test subset.\label{fig:snr}}
\end{figure}

\end{document}