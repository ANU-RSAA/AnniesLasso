%% This file is part of the Annie's Lasso project.
%% Copyright 2015 the authors.  All rights reserved.

% To-Do
% -----
% - make full outline
% - get to zeroth draft
% - audit for usage of vector and list
% - search for all occurrences of DWH, AC, or MKN in the text and fix them.
% - audit 'test-set star', 'training-set star', etc. should X-set be hyphenated in these circumstances?

% Style Notes
% -----------
% - Use \acronym{NASA} for acronyms like NASA.
% - The label list \ell is a *list*; the vectorization v(ell) is a *vector*.

\documentclass[12pt,preprint]{aastex}
\usepackage{amsmath,amssymb}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\TheCannon}{\project{The~Cannon}}
\newcommand{\tc}{\TheCannon}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\sdss}{\project{\acronym{SDSS-IV}}}
\newcommand{\apogee}{\project{\acronym{APOGEE}}}
\newcommand{\aspcap}{\project{\acronym{ASPCAP}}}
\newcommand{\dr}{\acronym{DR12}}
\newcommand{\logg}{\log g}
\newcommand{\mh}{\mathrm{[M/H]}}
\newcommand{\Teff}{T_{\mathrm{eff}}}
\newcommand{\Dvector}[1]{\boldsymbol{#1}}
\newcommand{\vectheta}{\Dvector{\theta}}
\newcommand{\vecv}{\Dvector{v}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}\,}

\begin{document}

\title{\textsl{The Cannon 2:} A data-driven model of stellar spectra \\
       for detailed chemical abundance analyses}
\author{Andrew~R.~Casey\altaffilmark{1},
        David~W.~Hogg\altaffilmark{2,3,4,5},
        Melissa~Ness\altaffilmark{5},
        Hans-Walter~Rix\altaffilmark{5},
        Daniel~Foreman-Mackey\altaffilmark{6,7},
    and~Gerry~Gilmore\altaffilmark{1}}
\altaffiltext{1}{Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge CB3~0HA, UK}    
\altaffiltext{2}{Simons Center for Data Analysis, 160 Fifth Avenue, 7th Floor, New York, NY 10010, USA}
\altaffiltext{3}{Center for Cosmology and Particle Physics, Department of Physics,
  New York University, 4 Washington Pl., room 424, New York, NY, 10003, USA}
\altaffiltext{4}{Center for Data Science, New York University, 726 Broadway, 7th Floor, New York, NY 10003, USA}
\altaffiltext{5}{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\altaffiltext{6}{Sagan Fellow}
\altaffiltext{7}{Department of Astronomy, University of Washington, Box 351580, Seattle, WA 98195-1580}
\email{arc@ast.cam.ac.uk}


\begin{abstract}
% Context
In previous work it has been shown that it is possible to train a generative
probabilistic model for stellar spectra using a training set of stars, each 
labeled with known physical attributes ($\Teff$, $\logg$, $\mh$), and then use 
that model to infer labels for unlabeled stars, even stars with lower 
signal-to-noise observations \citet{tc}.
% Aims
Here we ask whether this is possible when the dimensionality of the chemical
abundance space is large (15 abundances: C, N, O, Na, Mg, Al, Si, S, K, Ca, Ti, 
V, Mn, Fe, Ni) and the model is non-linear in its response to abundance and 
parameter changes.
% Methods
We adopt ideas from compressed sensing to limit overall model complexity (number
of non-zero model parameters) while retaining model freedom.  The training set 
consists of 12,681 red-giant stars with high signal-to-noise spectroscopic 
observations and stellar parameters and abundances taken from the \apogee\ 
Survey.
% Results
We find that we can successfully train and use a model with 17 stellar labels.
Validation shows that the model does a good job of inferring all 17 labels, even
when we degrade the signal-to-noise of the validation set by discarding some of
the spectroscopic observing time.  The model dependencies make sense; the 
derivatives of the spectral mean model with respect to abundances correlate well
with known atomic lines, and we are able to identify elements belonging to 
atomic lines that were previously unknown.  We deliver open-source code and also
stellar parameters and 15 abundances for a set of 98,462 stars.
\end{abstract}



\section{Introduction}
The detailed surface abundances of a star reflect its formation environment and
the supernova(e) that preceded it.  Minute perturbations in the properties of 
either can produce subtle chemical abundance differences on successive
generations of stars.  This chemical abundance signature largely remains 
unchanged in the photosphere throughout a star's lifetime, thereby providing a 
fossilised record of the gas composition during formation and an imprinted 
signature of stars that preceded it.  The variation between these signatures is 
small, implying that it is difficult to distinguish subtle chemical patterns 
from different formation sites.  However if they were distinguishable, a 
sufficiently large collection of precise stellar abundances would unambiguously
unravel the complete chemical evolution of the Milky Way.


There are some exceptions to this line of reasoning: the abundances of some
elements can (and do) change throughout a star's lifetime.  However these are
subtle signatures that are reflective of intriguing astrophysical phenomena:
atomic diffusion, evolutionary mixing, the presence (or accretion) of 
exoplanets, material transfer from a stellar binary companion, and countless
other scenarios.  In the same way that precise abundances are required to 
identify unique chemical formation signatures, precise abundances are necessary
to distinguish minute evolutionary or environment differences between subsets of
stars.  In these scenarios we argue most astrophysicists care less about
accuracy; it is the abundance \emph{precision} that imparts discovery and 
understanding of these astrophysical phenomena.  Indeed, in most studies of 
stellar astrophysics \textit{an increase in precision yields far more 
astrophysical insight than a comparable increase in accuracy}.


For these reasons a comprehensive catalog of precise detailed chemical 
abundances would revolutionise our understanding of planetary, stellar, and 
galactic astrophysics.  This goal has only recently become feasible given the 
increasing volume and quality of stellar spectra obtained in the last decade.  
Specifically, large surveys are obtaining high-resolution 
($\mathcal{R} \gtrsim 20,000$), high signal-to-noise (S/N) ratio spectra for
$\sim10^5$---$10^6$ stars across all components of the Galaxy.  This is a sharp
relative increase in data volume, which has mandated the automation of spectral
analysis, and encouraged dozens of groups to produce bespoke pipelines.


Most automated pipelines have grown from classical, manual methods.  That is to
say, there has been relatively little work on unconventional methods to analyse
spectra.  Spectroscopists have sought to hard-code their experience (or 
subjectivity), with arbitrary heuristics enforced for wavelength masks, 
convergence criteria, and similar analysis issues.  Most of these decisions are 
based on optimizing the resultant accuracy for a few stars with extremely high 
quality data (e.g., high S/N).  As a result, these heuristics are frequently 
incompatible for data with more modest (and representative) S/N ratios.  
Therefore, for most stars in the sample, it can be shown from repeat or blind 
experiments that that individual pipelines yield demonstrably imprecise 
abundances for data with more typical S/N ratios.  Moreover, results from 
individual pipelines vary significantly with respect to each other, with 
abundance differences an order of magnitude larger than any of the 
aforementioned astrophysical signatures of interest.  Thus, while there has been
substantial effort in automating classical analysis techniques, they are 
generally imprecise at modest S/N ratios, and yield imprecise or inaccurate 
results when compared against each other.


Considerable efforts have also been spent on improving the accuracy of physical
models.  Indeed it is impossible to measure physical properties of stars (or 
their chemical abundances) accurately without accurate physical models of 
stellar spectra.  However, physics-based models of stars have a number of known
problems.  The atmospheres are generally one-dimensional; three-dimensional
models remain computationally impractical for more than just a few stars.  As a
consequence of the limited atmosphere dimensionality, crude (knowingly 
incorrect) approximations for the convection and turbulence are necessary. 
Although grids of three-dimensional hydrodynamic models have been produced 
and averaged to one-dimensional approximations, these models assume local
thermodynamic equalibrium (LTE).  Properly accounting (or even approximating) 
departures from LTE is a formidable analytic and computational challenge.
Lastly, while laboratory efforts have thoroughly improved much of the faulty
atomic and molecular data, this process is unquestionably incomplete. For these
reasons there are some spectral features that are much better understood than
others.  As a consequence, physics-based methods are restricted to spectral
regions and parameter spaces that are understood marginally better, which vastly
limits their applicability and interpretability \emph{by construction}.


In detail, physics-based models do not explain all pixels of stellar 
spectroscopy at the precision with which we are currently observing.  The data
quality have outgrown the classical methods used to analyse them.  This led us 
to build \TheCannon\footnote{It is important (to us) to note that \TheCannon\ 
is named not after a weapon but instead after Annie Jump Cannon, who was the 
first to correctly order stellar spectra in temperature order \citep[and who did
so by looking at the data, and without any use of physics-based models, see][]
{Cannon_1911}.} (\citealt{tc}), a data-driven---as opposed to 
physics-based---model for stellar spectra.  \TheCannon\ is a data-driven model, 
but it differs from standard machine-learning approaches because it contains an 
explicit noise model. Given a representative set of stars with known labels of 
high-fidelity (see below), \TheCannon\ provides a generative model for stellar 
spectra based on a linear combination of the labels.  

%DWH: Cite some relevant literature.

Before we introduce the construction of the model and the tests we employed to
evaluate it, we first need to introduce the relevant terminology.  In all that 
follows, we will call stellar parameters ($\Teff$ and $\logg$) and the full set
of 15 chemical abundances collectively ``labels''.  This unifies and collapses 
the phrase ``stellar parameters and chemical abundances'' to a word, and 
connects to relevant terminology for supervised methods in the machine-learning
and statistics literatures.  Consider a spectroscopic survey that contains 
\emph{survey objects}.  Within that set of survey objects is a subset of stars 
where the data have high S/N ratios such that we can assert the labels are 
reported with high fidelity.  We will call this sample the \emph{labelled set}, 
whereas all other survey objects are categorized to the \emph{unlabelled set}.  
That is to say, although objects in the \emph{unlabelled set} may have reported 
labels (e.g., from a Survey pipeline), we are going to construct (train) 
\TheCannon\ using data in the labelled set and estimate (test) labels for those
in the unlabelled set.  In practice the data used for training is only a subset 
of the labelled set; we can assign a subset of stars in the labelled set that 
are not used for training, but are later used to confirm (validate) the 
predictive power of our model.  This means that \TheCannon\ can transfer labels
from high S/N training set stars to low S/N test-set stars; that is, the 
training set and the test set do not need to be statistically identical.  This 
is related to the fact that \TheCannon\ is an interpretable model; the internals
of the model are the dependencies of the spectral expectation and variance on
wavelength and physical parameters of the star. 


%DWH: Labels for data at wavelengths where there are no good labels.

%DWH: Labels that are consistent across surveys and wavelengths.

%DWH: Should we be saying things about the fact that the model is probabilistic?
%It takes as input stellar labels and gives as output a pdf for stellar spectra?

%DWH: The differences between measurement and prediction...


In the first work we did with \TheCannon, we only used a small number of labels
(three in the original work, and four or five in late work; \citealt{tc, age}). 
Here we were guided by thoughts related to density estimation: As the length of 
the label list $K$ grows, so too does the model complexity.  Sampling well a 
$K$-dimensional label space takes a training set the size of which scales 
exponentially (or worse) with $K$.  Subsequent experiments, however, did not 
bear this out. We found that we can transfer many labels from the training set 
to the test set, with training sets of thousands of stars.  The fundamental 
reason is that \TheCannon\ is \emph{not} a density estimator!  It is more like 
an \emph{interpolator}, which effectively finds stars in the training set that 
are close to the test star, and transfers labels, using the smooth polynomial 
model as a kind of regularizer.


The capacity to extend to a larger set of $K$ labels without significant
computational detriment offers tantalizing opportunities.  The most 
straightforward would be to include abundances of individual elements as labels.
However in doing so, it can be shown that a standard \emph{Cannon} model yields
coefficients (spectral derivatives; see next section) that are incompatible with 
expectations from physics: there may be an abundance label with non-zero 
contributions at all pixels, however physically we know that spectral lines of a
single element do not contribute at all wavelengths. 


For this reason alone we know that the problem is sparse.  Here we exploit this 
information to the fullest, using standard regularization methods to discover
and enforce sparsity. We consider the \emph{entire} 17-dimensional label space 
produced by the \apogee\ \aspcap\ pipeline.  We accept the \aspcap\ labels for 
a subset of the highest S/N stars, and adopt these stars and their labels as the
labelled set.  We show by validation that we can transfer these labels to much 
lower S/N stars, with reduced precision but no strong biases.  We then use the
system to label all of the stars in the \apogee\ \dr\ data set.  After
internally validating our model, we verify our abundance precision using tests
of globular and open clusters.



\section{Method}

\noindent{}We assume the following about \TheCannon\ and the \apogee\ \dr:
\begin{itemize}
\item
Stars with similar labels (parameters and abundances) have similar spectra.
\item
The expectation of the spectrum of a star is a smooth function of the values of 
the labels for that star.  Further than this, we assume that the function is so 
smooth it is reasonably approximated with a quadratic form in label space.
\item
The resolution of all \apogee\ spectra are identical, all spectra are calibrated
to the same rest wavelength grid, the noise is Gaussian and independent from 
pixel to pixel (with correctly known variances at the pixel level).  
Importantly, we are \emph{not} assuming that different stars have similar noise
variances, nor that the labelled and unlabelled sets have the same noise model.
\item
We have a training set of stars with accurate labels, where ``accurate'' is 
defined by the accuracy requirements of the output labels.  It might be more 
accurate to say that we are assuming that the training set stars have 
\emph{consistent} labels (consistent with the assumptions of smoothness, above).
\item
The training set is representative, in the sense that the training set stars 
span the label space similarly to how any test or prediction set spans the label
space.
\item
We assume our continuum normalization procedure (described below) is consistent.
Note that we do not require \emph{true} continuum-normalization in the classical
sense because any offset (even a label-dependent residual due to a strong 
absorption line) can be captured by the model.  Instead we require that our 
normalization procedure is invariant with respect to S/N ratios.
\end{itemize}


\noindent{}Given these assumptions, the model we adopt is
\begin{eqnarray}
  y_{jn} &=& \vecv(\ell_n)\cdot\vectheta_j + e_{jn}
  \label{eq:model}\quad ,
\end{eqnarray}
where $y_{jn}$ is the data for star $n$ at wavelength pixel $j$, $\vecv(\ell_n)$
is a function that takes as input the label list $\ell_n$ of length $K$ for star
$n$ and outputs a vector of length $D>K$ of functions of those labels,
$\vectheta_j$ is a vector of length $D$ of parameters controlling the model at 
wavelength pixel $j$, and $e_{jn}$ is a noise draw or residual.  We refer to 
$\vecv(\ell_n)$ as ``the vectorizing function'', which allows for arbitrarily 
complex functions that might not be simple polynomial expansions of the label 
list $\ell_n$ (e.g., sums of sines and cosines).  Inasmuch as the model is good,
the noise values $e_{jn}$ can be taken to be drawn from a Gaussian with zero 
mean and variance $\sigma^2_{jn}+s^2_j$, where $\sigma^2_{jn}$ is the 
pipeline-reported uncertainty variance on datum $y_{jn}$ and $s^2_j$ is a 
parameter describing excess variance at wavelength pixel $j$.


Two comments about the model (\ref{eq:model}).  The first is that, because the 
$e_{jn}$ are thought of as being drawn from a probability density function (pdf),
it is a probabilistic model for the spectral data $y_{jn}$.  The second is that
the output of the function $\vecv(\ell)$ can be thought of as a row of the 
``design matrix'' that defines the possible freedom given to the spectrum 
expectation model.


In the \emph{training step}, we fix the $K$-lists of labels $\ell_n$ for all 
training set stars $n$.  We seek, at each wavelength pixel $j$, the $[D+1]$ 
parameters $\vectheta_j,s^2_j$ that optimize a penalized likelihood:
\begin{eqnarray}\label{eq:train}
  \vectheta_j,s^2_j &\leftarrow& \argmin{\vectheta,s}\left[
    \sum_{n=0}^{N-1} \frac{[y_{jn}-\vecv(\ell_n)\cdot\vectheta]^2}{\sigma^2_{jn}+s^2}
    + \sum_{n=0}^{N-1} \ln(\sigma^2_{jn}+s^2)
    + \Lambda_j\,Q(\vectheta)
    \right]
  \quad ,
\end{eqnarray}
where $\Lambda_j$ is a regularization parameter, and $Q(\vectheta)$ is a 
regularizing function that encourages parameters to take on zero values.  The 
regularizing function takes a $D$-vector as input and returns a scalar value.
We call this penalized likelihood---the argument of the argmin in 
equation~(\ref{eq:train})---the \emph{training scalar}.  We will adopt for the 
regularizing function $Q(\vectheta)$ in the training scalar a modification of L1
regularization, discussed below.  Although the training-step optimization 
problem will not in general be convex, we can make choices for $Q(\vectheta)$ 
(and we will) to make the problem such that it would be convex at any fixed 
value of $s^2$; for this reason it will tend to optimize well in most cases of 
interest.


The regularization parameter $\Lambda_j$ sets the strength of the 
regularization; as $\Lambda_j$ increases, the number of non-zero components of 
the parameter vector $\vectheta_j$ will decrease.  We give the regularization 
parameter a subscript $j$ because in general we can set it differently at every
wavelength.  This makes sense, because different wavelengths have very different
dependences on different components of the label list $\ell$.  In practice the
value of $\Lambda_j$ should be set by full cross-validation, and possibly 
include restrictions based on physical arguments (e.g., a particular element 
does not have any spectral lines near this pixel, therefore the contributions
from this label must be zero).  This vastly expands the number of potential 
hyperparameters, and thus the computing expense required to determine them.  
Thus for the purpose of this work we will set a single value of $\Lambda$ (for 
all $j$ pixels) by validation.


In the \emph{test step}, we fix the parameters $\vectheta_j,s^2_j$ at all
wavelength pixels $j$.  We seek, for each test-set star $m$, the $K$-list of 
labels $\ell_m$ that optimizes the likelihood:
\begin{eqnarray}\label{eq:test}
  \ell_m &\leftarrow& \argmin{\ell}\left[
    \sum_{j=0}^{J-1} \frac{[y_{jm}-\vecv(\ell)\cdot\vectheta_j]^2}{\sigma^2_{jm}+s^2_j}
    \right]
  \quad .
\end{eqnarray}
If the vectorizing function $\vecv(\ell)$ is non-linear (as it is in our 
quadratic model), the test-step optimization is not convex.  However, the fact
that there are many pixels $j$ acting, each of which has a different functional
dependence on the labels in the label list $\ell$, tends to make the 
optimization find a good value for the label list $\ell$ in practice.  We call
this partial log likelihood---the argument of the argmin in 
equation~(\ref{eq:test})---the \emph{test scalar}.


The model freedom of \TheCannon\ is set by the vectorizing function 
$\vecv(\ell)$---which takes the $K$-element label list $\ell$ and expands it 
into a $D$-dimensional vector of components for the linear model---and the 
regularization $\Lambda_j\,Q(\vectheta)$.  Because we want the (simple, see 
below) regularization to treat the different parameters (the different 
components of $\vectheta$) in some sense ``equally'', we have to make sensible 
choices in the vectorizing function $\vecv(\ell)$.  One thing that the 
vectorizing function $\vecv(\ell)$ can do is offset the labels by some kind of
fiducial (mean, median, or other central) value, such that $\vecv(\ell)=0$ is at
a central location in the label space.  Another is to divide out a scale, 
because, for example, $\Teff$ values are in the thousands, but $\logg$ values
are of order unity.  If scale is not divided out, the (isotropic in $\vectheta$)
regularization will be much more harsh, effectively, on some parameters than 
others.  An extended version of this work might even consider different 
regularization terms for each $j$ pixel \emph{and} every label in $\ell$.


In what follows, we adopt the median value in the training set for each label 
value as the fiducial offset for that label.  We choose a dimensionless scale
factor $f$ times the label range (defined as the difference between the 97.5th
percentile and the 2.5th percentile of the training set along each label 
direction) as the scale for that label such that, for example, $\Teff$ values
are rescaled as:

% ARC do hat notation for rescaling?
\begin{eqnarray}\label{eq:label-norm}
  T_{{\rm eff},n} = \frac{T_{{\rm eff},n} - \langle\Teff\rangle}{f\cdot|p(\Teff,97.5) - p(\Teff,2.5)|}
\end{eqnarray}


\noindent{}For the regularizer $Q(\vectheta)$ we adopt a variant of L1
regularization; we set
\begin{eqnarray}
  Q(\vectheta) &=& \sum_{d=1}^{D-1} |\theta_d|
  \quad,
\end{eqnarray}
where the sum is over the $[D-1]$ components of $\vectheta$, excluding the 
zeroth component because we don't ever expect that component to vanish. 
(Forgive a notational similarity here: When we subscript $\vectheta$ with $j$ we
mean ``the $D$-vector of parameters associated with wavelength pixel $j$''; when
we subscript $\theta$ with $d$ we mean ``the single parameter along coordinate
axis $d$ in the $D$-dimensional $\vectheta$ vector space.)  There is a great 
deal of theory about this kind of regularization; it is called L1 or the LASSO
(DWH CITE STUFF).  
L1 regularization encourages parameters to vanish precisely but
doesn't break convexity for convex problems.

% SPACING UP TO HERE.

%DWH: To set the amplitude $\Lambda_j$ at each wavelength pixel $j$, we
%employ a validation of the following form:....

%DWH: Scaling argument that, in general, the best value of $\Lambda_j$
%should be on the same order as the number of stars in the training
%set....

%DWH: Note that the vectorizer scales $\Delta$ play into the
%regularization, because the bigger the $\Delta$, the more penalized
%the cross (quadratic) terms become relative to the linear terms...


\section{Training, validation, and test data}

We employ the \apogee\ Data Release 12 data to demonstrate the effectiveness of
\TheCannon\ and the regularization approach in a high-dimensional label space.
We constructed our labelled set (recall that the training and validation sets 
are subsets of the labelled set) using sensible criteria for stars in version 
\texttt{v603} from the \aspcap\ pipeline.  We employed several quality criteria 
to distill a robust yet representative labelled set: We first removed any stars
with problematic flags from the \aspcap\ pipeline, requiring 
\texttt{ASPCAPFLAG = 0}.  We excluded stars with S/N ratios outside the range of
200-300, and stars with a radial velocity scatter larger than 1~km~s$^{-1}$.  We
further demanded that our training set include reported abundances in all 15 
elements (C, N, O, Na, Mg, Al, Si, S, K, Ca, Ti, V, Mn, Fe, and Ni), and 
restricted the abundance range such that $2 > \mathrm{[X/Fe]} > -2$, 
$\mathrm{[Fe/H]} > -3$, and $[\alpha/\mathrm{Fe}] > -0.1$.  A visual comparison 
of the [V/H] labels with other (Fe-peak) abundance labels showed that many stars
in the training set had spurious measurements of [V/H].  For this reason we 
further constrained the labelled set such that $\mathrm{[V/Fe]} > -0.6$.  The 
distilled sample includes 14,141 red giant branch stars that will form our 
labelled set, with [Fe/H] labels ranging from $\mathrm{[Fe/H]} = [-2.10, 0.30]$. 


The \apogee\ \texttt{apStar} files contain rest-wavelength, resampled fluxes 
from individual visits for a given star, and an error array for those fluxes.
Although the \texttt{aspcapStar} files contain stacked, pseudo-continuum 
normalized spectra for a given star, we chose to only use the fluxes from the 
\texttt{apStar} data files throughout this work.  Our reasoning is as follows. 
The \aspcap\ pipeline uses a running filter window to determine the continuum.  
For this reason it is provably variant with S/N ratios: at low S/N ratios the 
inferred continuum will be systematically shifted with respect to the same star 
observed in high S/N.  Although our labelled set only includes high S/N spectra,
our results would suffer if we employed the \aspcap\ normalization procedure for
low S/N spectra in the test set.  Thus, we opted to normalize and stack the
fluxes from individual visits provided in the \texttt{apStar} files.


Individual spectra in the \texttt{apStar} files contain associated error arrays
for each observation, although the error arrays do not encapsulate all knowledge
about reduction issues.  Every pixel from a single observation contains a bitmask
flag that highlights BLAH, BLAH and BLAH.  While flagged, these issues are not
reflected by larger uncertainties for that particular pixel.  For this reason we
opted to combine information from the flags to produce more representative
inverse variance arrays.  Specifically we added  XYZ to the inverse variance
arrays for flags ABC to each individual spectrum.  


After updating the inverse variance arrays to account for flagged pixels, we
pseduo-continuum-normalized the individual observations, and later stacked all the
normalized spectra for every star to produce a single high S/N spectrum.
The continuum normalization was performed as follows.  We identified a distilled
list of continuum pixels from the spectra, and for each observation we fit those
pixels with a function of sines and cosines:



Using a function of sines and cosines for pseudo-continuum normalization has a
number of advantages over alternative functions used for continuum normalization.
Firstly, it is a linear operation on the spectrum fluxes.  Moreover, once the
design matrix X has been produced -- because the continuum pixels do not change
between observations -- the same design matrix can be used for many observations.
Therefore, the normalization is a cheap operation.  More importantly, the use of
sine and cosine functions implies that edges of the continuum function are going
to be bounded (more bounded than a polynomial function), and are therefore less
susceptible to issues with overfitting where the edge of the continuum function
will be unbounded.


It is important to note that our approach is \emph{pseudo}-continuum normalization.
While our choice of continuum pixels is well-informed, albeit arbitrary, this
procedure does not require that the chosen pixels are indeed true continuum
pixels.  The `continuum' pixels could be randomly selected, and as long as the
same pixels were used to normalize all stars, our approach would work equally
well: all spectra would be `normalized' in the same way, and the residual flux
behaviour from a `true'-normalized spectra would be captured by the spectral
model.  For this reason, we have only attempted to select well-informed 
continuum pixels in order to maximize model interpretability.  That is to say,
by using reasonable continuum pixels we can be sure that any spectral model
derivatives can be interpreted as being physically motivated, and less likely to
simply be capturing any residual continuum effects.


For all of the aforementioned reasons (invariance with respect to S/N, bounded
functions, linear operations, and repeatability between spectra), we emphasize
that if traditional continuum methods are employed, \TheCannon\ will (likely) 
give \emph{very bad results}.  Adopting a robust continuum normalization 
procedure is paramount.  After normalizing all individual spectra in the
\texttt{apStar} files, we re-stacked the spectra using the inverse variance of
each pixel (in each spectrum) as weights.  Therefore for all stars observed by
\apogee, we have normalized individual and combined spectra, allowing for 
explorations of label determination at low S/N (see later section).


The line spread function of \apogee\ spectra is wavelength- and fibre-dependent.
Given this information, our current implementation of \tc\ is knowingly 
sub-optimal: we assume nothing about differing line spread functions between 
stars in the labelled or unlabelled set. Similarly, we make no effort to 
accommodate fast-rotating stars, where the effect on the spectrum is 
approximately represented by convolution with a Gaussian kernel (in the same way
a lower resolution would be). 


%DWH: Define the various possible sets of ``labeled'' stars and the set of ``unlabeled''
%stars, for our various purposes.

%DWH: Any training set must be a subset of the
%labeled stars.
%We are going to use different subsets for different trainings of \TheCannon.

%To demonstrate the predictive power and accuracy of the method, we
%will (below) perform a conservative cross-validation using cyclic
%training, validation, and test subsets of the labeled data:
%To each star in the input set of labeled stars (the superset of any
%possible training set we might use), we assign an integer $0\leq
%q<10$, with uniform probability across all 10 possible values of $q$.
%This permits a very conservative 10-fold cross-validation, in which we
%choose $8/10$ of the labeled data as the training set, a disjoint
%$1/10$ as a validation set for choosing the hyper-parameters
%$\Lambda_j$, and a mutually disjoint $1/10$ as a test set for
%prediction.
%This train--validate--test framework is very conservative, because there
%is no way, for each choice of train, validate, and test sets from the
%labeled data, for the test set to influence the choice of
%hyper-parameters, or indeed for any information to flow from the test
%set into the training.

%When \TheCannon\ is used to label the full unlabeled set, we train on the entire
%unlabled data set (the largest possible training set) and
%use values for the hyper-parameters $\Lambda_j$ derived from the
%10-fold cross-validation experiment, as we will describe below.

% ARC: Figure: HRD (coloured by [Fe/H]) of the labelled set.

\section{Experiments}

\paragraph{Validation for hyper-parameter selection:}
Hello World!

\paragraph{Three-label case:}
Hello World!

\paragraph{Seventeen-label case:}
Hello World!

\paragraph{Label recovery as a function of signal-to-noise:}
Hello World!

\section{Results}

DWH: Do we need this section?

DWh: What can we say about chemical abundance precision and accuracy?

DWH: What can we say about chemical abundance space?

DWH: What can we say about the promise of chemical tagging?

\section{Discussion}


\TheCannon\ is a prediction system: It is designed to predict the labels for 
stars.  It has a fundamental assumption that the training set is statistically
identical to the prediction/test set.  If there are covariances in the label 
space---that is, if stars in the training set are high in label $X$ whenever
they are also high in label $Y$---then there will be opportunities for 
\TheCannon\ to get information about $Y$ from considering data aspects that 
depend on $X$ (which, according to the training set, is reliably connected to 
$Y$).  For this reason, we expect that, given a finite training set, there will
be situations in which lines from one element are used to predict the abundance
of another.  A good example is shown in \figurename~\ref{fig:whatever}, where we
see WHATEVER.

DWH: Yada Yada...


ARC: The fact that we do so badly for [V/H], yet [V/H] is empirically covariant with [Fe/H] in the same way that other Fe-peak abundance labels are, tells us that there is a fundamental limit to how much empirically covariant information is entering into our results!


DWH: No accounting for variable spectroscopic resolution, nor
microturbulence nor rotation.

DWH:  Once again, the differences between measurement and prediction...

DWH:  Why is this going to change the World nonetheless?

DWH: All of the code for this project is available with documentation
at \url{http://thecannon.io/}.

\acknowledgements
Some people...
grants...
\sdss...
astropy... topcat... ads...

\begin{thebibliography}{dummy}\raggedright
\bibitem[Ness et al.(2015a)]{tc} Ness, M., Hogg, D.~W., 
Rix, H.-W., Ho, A.~Y.~Q., \& Zasowski, G.\ 2015, \apj, 808, 16
\bibitem[Ness et al.(2015b)]{age} Ness, M., Hogg, D.~W., 
Rix, H., et al.\ 2015, arXiv:1511.08204 
\end{thebibliography}

\clearpage

\begin{figure}[p]
\caption{Choose two wavelengths (one continuum and one interesting)
  and plot some scatter plots of flux vs various parameters, and also
  cross-validation results for the
  hyper-parameters.\label{fig:onewavelength}}
\end{figure}

\begin{figure}[p]
\caption{Something about hyper-parameters and scatter as a function of
  wavelength.\label{fig:hyperpars}}
\end{figure}

\begin{figure}[p]
\caption{Something about first derivatives of the spectral expectation
  with respect to labels as a function of
  wavelength.\label{fig:derivatives}}
\end{figure}

\begin{figure}[p]
\caption{Show a few sample spectra and demonstrate that the quality of
  the model prediction is extremely good; better than
  \aspcap.\label{fig:correctness}}
\end{figure}

\begin{figure}[p]
\caption{Something showing that our results on the full unlabeled set
  make sense and seem correct!\label{fig:fulltest}}
\end{figure}

\begin{figure}[p]
\caption{Choose two elements that are interesting, and pull apart the
  differences between what we have and what \aspcap\ has for those
  elements.  We are better!\label{fig:elements}}
\end{figure}

\begin{figure}[p]
\caption{Something about the quality of results in the
  cross-validation as a function of the signal-to-noise of what we
  give in the test subset.\label{fig:snr}}
\end{figure}


\begin{figure}[p]
\includegraphics[width=\textwidth]{sparse-first-order-coefficients.pdf}
\caption{Normalized (to the maximum derivative value at any $\theta$) first-order derivatives for [Al/H], [S/H], and [K/H] from a regularized Cannon model with $\Lambda = 10^{3.5}$ and $f = 20$. For the sake of clarity, we have only shown $\theta$ values that exceed the shaded region. The top markings for K and Al correspond to the lines used by Smith et al. The black markers indicate 'Missing' or 'Unknown' lines in the APOGEE spectra according to Shetrone et al. Here we identify these elements from our model derivatives.  \label{fig:inferring-lines}}
\end{figure}

\begin{figure}[p]
\includegraphics[width=\textwidth]{sparse-first-order-coefficients-zoom.pdf}
\caption{Zoom in of previous figure, showing the theta values for all elements. Al and S are highlighted and colored as per Figure \ref{fig:inferring-lines}.\label{fig:inferring-lines2}}
\end{figure}


\begin{figure}[p]
\includegraphics[width=\textwidth]{sparsity.pdf}
\caption{The fraction of non-zero coefficients (sparsity) of 17-label models with different scale factors $f$ and regularization terms $\Lambda$.  The sparsity of the first-order coefficients are shown in the left panel, the second-order coefficients (e.g., $\Teff^2$, $\Teff\cdot\logg$, or $[\rm{Al}/\rm{H}]\cdot[\rm{Mn}/\rm{H}]$) in the middle panel, and total sparsity (excluding the baseline spectrum coefficient $\theta_0$) in the right-hand panel.\label{fig:sparsity}}
\end{figure}


\end{document}
