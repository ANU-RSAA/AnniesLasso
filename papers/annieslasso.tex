%% This file is part of the Annie's Lasso project.
%% Copyright 2015 the authors.  All rights reserved.

% To-Do
% -----
% - get to zeroth draft
% - search for all occurrences of DWH, AC, or MKN in the text and fix them.

% Style Notes
% -----------
% - Use \acronym{NASA} for acronyms like NASA.

\documentclass[12pt,preprint]{aastex}
\usepackage{amsmath,amssymb}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\thecannon}{\project{The~Cannon}}
\newcommand{\acronym}[1]{\small{#1}}
\newcommand{\sdss}{\project{\acronym{SDSS-IV}}}
\newcommand{\apogee}{\project{\acronym{APOGEE2}}}
\newcommand{\aspcap}{\project{\acronym{ASPCAP}}}
\newcommand{\dr}{\acronym{DRXX}}
\newcommand{\logg}{\log g}
\newcommand{\Teff}{T_{\mathrm{eff}}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}\,}

\begin{document}

\title{\textsl{The Cannon 2:} A data-driven model \\ for detailed chemical abundance analyses}
\author{AC, DWH, MKN, others}

\begin{abstract}
% context
In previous work, we have shown that it is possible to train a generative
probabilistic model for stellar spectra using a training set of stars, each with known
parameters ($\logg$ and $\Teff$) and chemical abundance, and then use that model to infer labels for
unlabeled stars, even stars with lower signal-to-noise observations.
% aims
Here we ask whether this is possible when the dimensionality of the chemical
abundance space is large (15 abundances: Mg, Fe, etc, etc, Al)
and the model is non-linear in its response to abundance and parameter changes.
% method
We adopt ideas from compressed sensing to limit overall model complexity (number
of non-zero parameters) while retaining model freedom.
The training set is a set of YYY red-giant stars with high signal-to-noise
spectroscopic observations and stellar parameters and abundances taken from the
\apogee\ Survey.
% results
We find that we can successfully train and use a model with 17 stellar labels.
Cross-validation shows that the model does a good job of inferring all 17 labels
(with the exception of XXX), even when we degrade the signal-to-noise of the
validation set by discarding some of the spectroscopic observing time.
The model dependencies make sense; the derivatives of the spectral mean model
with respect to abundances correlate well with known atomic lines.
We deliver open-source code and also stellar parameters and 15 abundances for a
set of ZZZ stars.
\end{abstract}

\section{Introduction}

We built \thecannon, and it rocked.
DWH: Summarize what it does and what it can be used for.
Cite some relevant literature.

DWH: Define the word ``label''.

DWH: Define the words ``train'', ``validate'', and ``test''.

DWH: Labels for data at wavelengths where there are no good labels.

DWH: Labels that are consistent across surveys and wavelengths.

\thecannon\ is a data-driven model, but it differs from standard machine-learning
approaches because it contains an explicit noise model.
This means that it can transfer labels from high signal-to-noise training-set
stars to low signal-to-noise test-set stars; that is, the training set and 
the test set do not need to be statistically identical.
This is related to the fact that \thecannon\ is an interpretable model;
the internals of the model are the dependencies of the spectral expetation
and variance on wavelength and physical parameters of the star.

DWH: Should we be saying things about the fact that the model is probabilistic?
It takes as input stellar labels and gives as output a pdf for stellar spectra?

DWH: In the first work we did with \thecannon, we only used a small
number of labels (three in the original work, and four or five in late
work; CITE).
Here we were guided by thoughts related to density estimation:
Sampling well a $K$-dimensional label space takes a training set the
size of which scales exponentially (or worse) with $K$.
Subsequent experiments, however, did not bear this out:
We found that we can transfer many labels from the training set to
the test set, with training sets of thousands of stars.
The fundamental reason is that \thecannon\ is \emph{not} a density estimator!
It is more like an \emph{interpolator}, which effectively finds stars in
the training set that are close to the test star, and transfers labels,
using the smooth polynomial model as a kind of regularizer.

DWH: Here we exploit this to the fullest.
We consider the \emph{entire} 17-dimensional label spece produced by
the \apogee\ \aspcap\ pipeline.
We (for very good reasons) believe the \aspcap\ labels for the highest
signal-to-noise stars, and adopt these stars and their labels as the
training set.
We show, by very conservative cross-validation, that we can transfer these
labels to much lower signal-to-noise stars, with reduced precision but no
strong biases.
We then use the system to label all of the stars in the \apogee\ \dr\ data set.

DWH: Etc!

\section{Method}

DWH: What are we going to assume?

The model is
\begin{eqnarray}
  y_{jn} &=& v(\ell_n)\cdot\theta_j + e_{jn}
  \label{eq:model}\quad ,
\end{eqnarray}
where $y_{jm}$ is the data for star $n$ at wavelength pixel $j$,
$v(\ell_n)$ is a function that takes as input
the label vector $\ell_n$ of length $K$ for star $n$
and outputs a vector of length $D>K$,
$\theta_j$ is a vector of length $D$ of parameters controlling the model at wavelength pixel $j$,
and $e_{jn}$ is a noise draw or residual.
Inasmuch as the model is good, the noise values $e_{jn}$ can be taken to be
drawn from a Gaussian with zero mean and variance $\sigma^2_{jn}+s^2_j$,
where $\sigma^2_{jn}$ is the pipeline-reported uncertainty variance on datum
$y_{jn}$ and $s^2_j$ is a parameter describing excess variance at wavelength pixel $j$.

Two comments about the model (\ref{eq:model}).
The first is that, because the $e_{jn}$ are thought of as being drawn from a 
probability density function (pdf), it is a probabilistic model for the spectral
data $y_{jn}$.
The second is that the output of the function $v(\ell)$ can be thought
of as a row of the ``design matrix'' that defines the possible freedom
given to the spectrum expectation model.

In the \emph{training step}, we fix the $K$-vectors of labels $\ell_n$
for all training-set stars $n$.
We seek, at each wavelength pixel $j$, the $[D+1]$ parameters
$\theta_j,s^2_j$ that optimize a penalized likelihood:
\begin{eqnarray}
  \theta_j,s^2_j &\leftarrow& \argmin{\theta,s^2}\left[
    \sum_{n=1}^N \frac{y_{jn}-v(\ell_n)\cdot\theta}{\sigma^2_{jn}+s^2}
    + \sum_{n=1}^N \ln(\sigma^2_{jn}+s^2)
    + \Lambda_j\,||\theta||_1^1
    \right]
  \\
  ||\theta||_1^1 &\equiv& \sum_{d=1}^D |\theta_d|
  \quad ,
\end{eqnarray}
where $\Lambda_j$ is a regularization parameter, and $||\theta||_1^1$ is
the L1-norm of $\theta$ or the sum of the absolute values of the
components $\theta_d$ of the $D$-vector $\theta$.
Although the training-step optimization problem is not convex, it
would be convex at any fixed value of $s^2$; for this reason it will
tend to optimize well in most cases of interest.

The regularization parameter $\Lambda_j$ sets the strength of the
regularization; as $\Lambda_j$ increases, the number of non-zero
components of the parameter vector $\theta_j$ will decrease.
We give the regularization parameter a subscript $j$ because in
general we can set it differently at every wavelength.
This makes sense, because different wavelengths have very different
dependences on different components of the label vector $\ell$.
In what follows, we will set the value of the $\Lambda_j$ by
validation or cross-validation or possibly based on physical arguments
about what matters!

In the \emph{test step}, we fix the parameters $\theta_j,s^2_j$ at all
wavelength pixels $j$.
We seek, for each test-set star $m$, the $K$-vector of labels $\ell_m$
that optimizes the likelihood:
\begin{eqnarray}
  \ell_m &\leftarrow& \argmin{\ell}\left[
    \sum_{j=1}^J \frac{y_{jm}-v(\ell)\cdot\theta_j}{\sigma^2_{jm}+s^2_j}
    \right]
  \quad .
\end{eqnarray}
Because the vector-to-vector function $v(\ell)$ is non-linear, the
test-step optimization is not convex.
However, the fact that there are many pixels $j$ acting, each of which
has a different functional dependence on $\ell$, tends to make the
optimization (in practice) find a good value for $\ell$.

\section{Training, validation, and test data}

MKN: Describe \apogee\ data here.

MKN: What is different about our continuum normalization from everyone else's?

DWH: Define the set of ``labeled'' stars and the set of ``unlabeled''
stars, for our purposes.  Any training set must be a subset of the
labeled stars.
We are going to use different subsets for different trainings of \thecannon.

To demonstrate the predictive power and accuracy of the method, we
will (below) perform a conservative cross-validation using cyclic
training, validation, and test subsets of the labeled data:
To each star in the input set of labeled stars (the superset of any
possible training set we might use), we assign an integer $0\leq
q<10$, with uniform probability across all 10 possible values of $q$.
This permits a very conservative 10-fold cross-validation, in which we
choose $8/10$ of the labeled data as the training set, a disjoint
$1/10$ as a validation set for choosing the hyper-parameters
$\Lambda_j$, and a mutually disjoint $1/10$ as a test set for
prediction.
This train--validate--test framework is very conservative, because there
is no way, for each choice of train, validate, and test sets from the
labeled data, for the test set to influence the choice of
hyper-parameters.

When \thecannon\ is used to label the full unlabeled set, we train on
the entire unlabled data set (the largest possible training set) and
use values for the hyper-parameters $\Lambda_j$ derived from the
10-fold cross-validation experiment, as we will describe below.

\section{Experiments}

\paragraph{Conservative cross-validation:}
Hello World!

\paragraph{Full train-and-test:}
Hello World!

\paragraph{Low signal-to-noise test:}
Hello World!

\section{Discussion}

DWh:  Hello World!  What were our assumptions and how would things have
changed if we had changed or relaxed them?

DWH:  What is bad or limited about what we have done?

DWH:  Why is this going to change the World nonetheless?

\acknowledgements
DWH: Dan Foreman-Mackey (UW) and other people...
grants...
\sdss...

\clearpage

\begin{figure}[p]
\caption{Choose two wavelengths (one continuum and one interesting)
  and plot some scatter plots of flux vs various parameters, and also
  cross-validation results for the
  hyper-parameters.\label{fig:onewavelength}}
\end{figure}

\begin{figure}[p]
\caption{Something about hyper-parameters and scatter as a function of
  wavelength.\label{fig:hyperpars}}
\end{figure}

\begin{figure}[p]
\caption{Something about first derivatives of the spectral expectation
  with respect to labels as a function of
  wavelength.\label{fig:derivatives}}
\end{figure}

\begin{figure}[p]
\caption{Show a few sample spectra and demonstrate that the quality of
  the model prediction is extremely good; better than
  \aspcap.\label{fig:correctness}}
\end{figure}

\begin{figure}[p]
\caption{Something showing that our results on the full unlabeled set
  make sense and seem correct!\label{fig:fulltest}}
\end{figure}

\begin{figure}[p]
\caption{Choose two elements that are interesting, and pull apart the
  differences between what we have and what \aspcap\ has for those
  elements.  We are better!\label{fig:elements}}
\end{figure}

\begin{figure}[p]
\caption{Something about the quality of results in the
  cross-validation as a function of the signal-to-noise of what we
  give in the test subset.\label{fig:snr}}
\end{figure}

\end{document}
