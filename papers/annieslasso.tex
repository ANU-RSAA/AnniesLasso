%% This file is part of the Annie's Lasso project.
%% Copyright 2015 the authors.  All rights reserved.

% To-Do
% -----
% - make full outline
% - get to zeroth draft
% - audit for usage of vector and list
% - search for all occurrences of DWH, AC, or MKN in the text and fix them.

% Style Notes
% -----------
% - Use \acronym{NASA} for acronyms like NASA.
% - The label list \ell is a *list*; the vectorization v(ell) is a *vector*.

\documentclass[12pt,preprint]{aastex}
\usepackage{amsmath,amssymb}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\TheCannon}{\project{The~Cannon}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\sdss}{\project{\acronym{SDSS-IV}}}
\newcommand{\apogee}{\project{\acronym{APOGEE2}}}
\newcommand{\aspcap}{\project{\acronym{ASPCAP}}}
\newcommand{\dr}{\acronym{DRXX}}
\newcommand{\logg}{\log g}
\newcommand{\Teff}{T_{\mathrm{eff}}}
\newcommand{\Dvector}[1]{\boldsymbol{#1}}
\newcommand{\vectheta}{\Dvector{\theta}}
\newcommand{\vecv}{\Dvector{v}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}\,}

\begin{document}

\title{\textsl{The Cannon 2:} A data-driven model of stellar spectra \\
       for detailed chemical abundance analyses}
\author{AC, DWH, MKN, others}

\begin{abstract}
% context
In previous work, we have shown that it is possible to train a generative
probabilistic model for stellar spectra using a training set of stars, each with known
parameters ($\logg$ and $\Teff$) and metallicity, and then use that model to infer labels for
unlabeled stars, even stars with lower signal-to-noise observations.
% aims
Here we ask whether this is possible when the dimensionality of the chemical
abundance space is large (15 abundances: Mg, Fe, etc, etc, Al)
and the model is non-linear in its response to abundance and parameter changes.
% method
We adopt ideas from compressed sensing to limit overall model complexity (number
of non-zero parameters) while retaining model freedom.
The training set is a set of YYY red-giant stars with high signal-to-noise
spectroscopic observations and stellar parameters and abundances taken from the
\apogee\ Survey.
% results
We find that we can successfully train and use a model with 17 stellar labels.
Cross-validation shows that the model does a good job of inferring all 17 labels
(with the exception of XXX), even when we degrade the signal-to-noise of the
validation set by discarding some of the spectroscopic observing time.
The model dependencies make sense; the derivatives of the spectral mean model
with respect to abundances correlate well with known atomic lines.
We deliver open-source code and also stellar parameters and 15 abundances for a
set of ZZZ stars.
\end{abstract}

\section{Introduction}

It is impossible to make accurate measurements of the physical
properties of stars, or their chemical abundances, without accurate
physical models of stellar spectra.
However, the physics-based models for stars have a number of known
problems:
They are generally one-dimensional rather than three-dimensional, and
either don't include or else have only effective models for some of
the relevant physical processes.
The input atomic and molecular physics information is faulty and
incomplete.
There are some wavelengths and lines that are much better understood
than others.
In detail, the models do not explain all pixels of stellar
spectroscopy at the precision with which we are currently observing,
which is signal-to-noise ratios in the hundreds for thousands or tens
of thousands of wavelength pixels per star.

This led us to build \TheCannon\ (\citealt{tc}), a data-driven---as
opposed to physics-based---model for stellar spectra.

DWH: Summarize what it does and what it can be used for.
DWH: Mention some users and projects, past, current, and future.
DWH: Cite some relevant literature.
It is important (to us) to note that \TheCannon\ is named not after a
weapon but instead after Annie Jump Cannon, who was the first to
correctly order stellar spectra in temperature order (and who did so
by looking at the data, and without any use of physics-based models).

In all that follows, we will call stellar parameters ($\Teff$ and
$\logg$) and the full set of 15 [CHECK] chemical abundances
collectively ``labels''.
This unifies and collapses the ``stellar parameters and chemical
abundances'' phrase to a word, and also connects to the terminology
for supervised methods in the machine-learning and statistics
literatures.

DWH: Define the words ``train'', ``validate'', and ``test''.  And maybe ``predict'' too?

DWH: Labels for data at wavelengths where there are no good labels.

DWH: Labels that are consistent across surveys and wavelengths.

\TheCannon\ is a data-driven model, but it differs from standard machine-learning
approaches because it contains an explicit noise model.
This means that it can transfer labels from high signal-to-noise training-set
stars to low signal-to-noise test-set stars; that is, the training set and 
the test set do not need to be statistically identical.
This is related to the fact that \TheCannon\ is an interpretable model;
the internals of the model are the dependencies of the spectral expetation
and variance on wavelength and physical parameters of the star.

DWH: Should we be saying things about the fact that the model is probabilistic?
It takes as input stellar labels and gives as output a pdf for stellar spectra?

DWH: In the first work we did with \TheCannon, we only used a small
number of labels (three in the original work, and four or five in late
work; \citealt{tc, age}).
Here we were guided by thoughts related to density estimation:
Sampling well a $K$-dimensional label space takes a training set the
size of which scales exponentially (or worse) with $K$.
Subsequent experiments, however, did not bear this out:
We found that we can transfer many labels from the training set to
the test set, with training sets of thousands of stars.
The fundamental reason is that \TheCannon\ is \emph{not} a density estimator!
It is more like an \emph{interpolator}, which effectively finds stars in
the training set that are close to the test star, and transfers labels,
using the smooth polynomial model as a kind of regularizer.

DWH: Here we exploit this to the fullest.
We consider the \emph{entire} 17-dimensional label spece produced by
the \apogee\ \aspcap\ pipeline.
We (for very good reasons) believe the \aspcap\ labels for the highest
signal-to-noise stars, and adopt these stars and their labels as the
training set.
We show, by very conservative cross-validation, that we can transfer these
labels to much lower signal-to-noise stars, with reduced precision but no
strong biases.
We then use the system to label all of the stars in the \apogee\ \dr\ data set.

DWH: The differences between measurement and prediction...

DWH: Etc!

\section{Method}

DWH: What are we going to assume?

The model is
\begin{eqnarray}
  y_{jn} &=& \vecv(\ell_n)\cdot\vectheta_j + e_{jn}
  \label{eq:model}\quad ,
\end{eqnarray}
where $y_{jn}$ is the data for star $n$ at wavelength pixel $j$,
$\vecv(\ell_n)$ is a function that takes as input
the label list $\ell_n$ of length $K$ for star $n$
and outputs a vector of length $D>K$ of functions of those labels,
$\vectheta_j$ is a vector of length $D$ of parameters controlling the model at wavelength pixel $j$,
and $e_{jn}$ is a noise draw or residual.
Inasmuch as the model is good, the noise values $e_{jn}$ can be taken to be
drawn from a Gaussian with zero mean and variance $\sigma^2_{jn}+s^2_j$,
where $\sigma^2_{jn}$ is the pipeline-reported uncertainty variance on datum
$y_{jn}$ and $s^2_j$ is a parameter describing excess variance at wavelength pixel $j$.

Two comments about the model (\ref{eq:model}).
The first is that, because the $e_{jn}$ are thought of as being drawn from a 
probability density function (pdf), it is a probabilistic model for the spectral
data $y_{jn}$.
The second is that the output of the function $\vecv(\ell)$ can be thought
of as a row of the ``design matrix'' that defines the possible freedom
given to the spectrum expectation model.

In the \emph{training step}, we fix the $K$-lists of labels $\ell_n$
for all training-set stars $n$.
We seek, at each wavelength pixel $j$, the $[D+1]$ parameters
$\vectheta_j,s^2_j$ that optimize a penalized likelihood:
\begin{eqnarray}
  \vectheta_j,s^2_j &\leftarrow& \argmin{\vectheta,s^2}\left[
    \sum_{n=0}^{N-1} \frac{[y_{jn}-\vecv(\ell_n)\cdot\vectheta]^2}{\sigma^2_{jn}+s^2}
    + \sum_{n=0}^{N-1} \ln(\sigma^2_{jn}+s^2)
    + \Lambda_j\,Q(\vectheta)
    \right]
  \quad ,
\end{eqnarray}
where $\Lambda_j$ is a regularization parameter, and $Q(\vectheta)$ is a 
regularizing function that encourages parameters to take on zero values.
The regularizing function takes a $D$-vector as input and returns a
scalar.
We will adopt for $Q(\vectheta)$ a modification of L1 regularization,
discussed below.
Although the training-step optimization problem will not in general be
convex, we can make choices for $Q(\vectheta)$ (and we will) to make the
problem such that it would be convex at any fixed value of $s^2$; for
this reason it will tend to optimize well in most cases of interest.

The regularization parameter $\Lambda_j$ sets the strength of the
regularization; as $\Lambda_j$ increases, the number of non-zero
components of the parameter vector $\vectheta_j$ will decrease.
We give the regularization parameter a subscript $j$ because in
general we can set it differently at every wavelength.
This makes sense, because different wavelengths have very different
dependences on different components of the label list $\ell$.
In what follows, we will set the value of the $\Lambda_j$ by
validation or cross-validation or possibly based on physical arguments
about what matters!

In the \emph{test step}, we fix the parameters $\vectheta_j,s^2_j$ at all
wavelength pixels $j$.
We seek, for each test-set star $m$, the $K$-list of labels $\ell_m$
that optimizes the likelihood:
\begin{eqnarray}
  \ell_m &\leftarrow& \argmin{\ell}\left[
    \sum_{j=0}^{J-1} \frac{[y_{jm}-\vecv(\ell)\cdot\vectheta_j]^2}{\sigma^2_{jm}+s^2_j}
    \right]
  \quad .
\end{eqnarray}
Because the vectorizing function $\vecv(\ell)$ is non-linear, the
test-step optimization is not convex.
However, the fact that there are many pixels $j$ acting, each of which
has a different functional dependence on the labels in the label list
$\ell$, tends to make the optimization (in practice) find a good value
for the label list $\ell$.

The model freedom of \TheCannon\ is set by the vectorizing function
$\vecv(\ell)$---which takes the $K$-element label list $\ell$ and expands
it into a $D$-dimensional vector of components for the linear
model---and the regularization $\Lambda_j\,Q(\vectheta)$.
Because we want the (simple, see below) regularization to treat the
different parameters (the different components of $\vectheta$) in some
sense ``equally'', we have to make sensible choices in the vectorizing
function $\vecv(\ell)$.
One thing that the vectorizing function $\vecv(\ell)$ can do is offset the
labels by some kind of fiducial (mean, median, or other central)
value, such that $\vecv(\ell)=0$ is at a central location in the label
space.
Another is divide out a scale, because, for example, $\Teff$ values
are in the thousands, but $\logg$ values are of order unity.
If scale is not divided out, the (isotropic in $\vectheta$)
regularization will be much more harsh, effectively, on some
parameters than others.

DWH: In detail we construct the vectorizing function $\vecv(\ell)$ as follows...

For the regularizer $Q(\vectheta)$ we adopt a variant of L1
regularization; we set
\begin{eqnarray}
  Q(\vectheta) &=& \sum_{d=1}^{D-1} |\theta_d|
  \quad,
\end{eqnarray}
where the sum is over the $[D-1]$ components of $\vectheta$, excluding
the zeroth component (because we don't ever expect that component to
vanish).
(Forgive a notational similarity here: When we subscript $\vectheta$ with
$j$ we mean ``the $D$-vector of parameters associated with wavelength
pixel $j$''; when we subscript $\theta$ with $d$ we mean ``the single
parameter along coordinate axis $d$ in the $D$-dimensional $\vectheta$
vector space.)
There is a great deal of theory about this kind of regularization; it
is called L1 or the lasso (DWH CITE STUFF).
L1 regularization encourages parameters to vanish precisely but
doesn't break convexity for convex problems.

DWH: To set the amplitude $\Lambda_j$ at each wavelength pixel $j$, we
employ cross-validation...

\section{Training, validation, and test data}

MKN: Describe \apogee\ data here.

MKN: What is different about our continuum normalization from everyone else's?

DWH: Define the set of ``labeled'' stars and the set of ``unlabeled''
stars, for our purposes.  Any training set must be a subset of the
labeled stars.
We are going to use different subsets for different trainings of \TheCannon.

To demonstrate the predictive power and accuracy of the method, we
will (below) perform a conservative cross-validation using cyclic
training, validation, and test subsets of the labeled data:
To each star in the input set of labeled stars (the superset of any
possible training set we might use), we assign an integer $0\leq
q<10$, with uniform probability across all 10 possible values of $q$.
This permits a very conservative 10-fold cross-validation, in which we
choose $8/10$ of the labeled data as the training set, a disjoint
$1/10$ as a validation set for choosing the hyper-parameters
$\Lambda_j$, and a mutually disjoint $1/10$ as a test set for
prediction.
This train--validate--test framework is very conservative, because there
is no way, for each choice of train, validate, and test sets from the
labeled data, for the test set to influence the choice of
hyper-parameters, or indeed for any information to flow from the test
set into the training.

When \TheCannon\ is used to label the full unlabeled set, we train on
the entire unlabled data set (the largest possible training set) and
use values for the hyper-parameters $\Lambda_j$ derived from the
10-fold cross-validation experiment, as we will describe below.

\section{Experiments}

\paragraph{Conservative cross-validation:}
Hello World!

\paragraph{Full train-and-test:}
Hello World!

\paragraph{Low signal-to-noise test:}
Hello World!

\section{Discussion}

DWh:  Hello World!  What were our assumptions and how would things have
changed if we had changed or relaxed them?

DWH:  What is bad or limited about what we have done?

DWH: No accounting for variable spectroscopic resolution, nor
microturbulence nor rotation.

DWH:  Once again, the differences between measurement and prediction...

DWH:  Why is this going to change the World nonetheless?

DWH: All of the code for this project is available with documentation
at \url{http://thecannon.io/}.

\acknowledgements
DWH: Dan Foreman-Mackey (UW) and Hans-Walter Rix (MPIA) and other people...
grants...
\sdss...

\begin{thebibliography}{dummy}\raggedright
\bibitem[Ness et al.(2015a)]{tc} Ness, M., Hogg, D.~W., 
Rix, H.-W., Ho, A.~Y.~Q., \& Zasowski, G.\ 2015, \apj, 808, 16
\bibitem[Ness et al.(2015b)]{age} Ness, M., Hogg, D.~W., 
Rix, H., et al.\ 2015, arXiv:1511.08204 
\end{thebibliography}

\clearpage

\begin{figure}[p]
\caption{Choose two wavelengths (one continuum and one interesting)
  and plot some scatter plots of flux vs various parameters, and also
  cross-validation results for the
  hyper-parameters.\label{fig:onewavelength}}
\end{figure}

\begin{figure}[p]
\caption{Something about hyper-parameters and scatter as a function of
  wavelength.\label{fig:hyperpars}}
\end{figure}

\begin{figure}[p]
\caption{Something about first derivatives of the spectral expectation
  with respect to labels as a function of
  wavelength.\label{fig:derivatives}}
\end{figure}

\begin{figure}[p]
\caption{Show a few sample spectra and demonstrate that the quality of
  the model prediction is extremely good; better than
  \aspcap.\label{fig:correctness}}
\end{figure}

\begin{figure}[p]
\caption{Something showing that our results on the full unlabeled set
  make sense and seem correct!\label{fig:fulltest}}
\end{figure}

\begin{figure}[p]
\caption{Choose two elements that are interesting, and pull apart the
  differences between what we have and what \aspcap\ has for those
  elements.  We are better!\label{fig:elements}}
\end{figure}

\begin{figure}[p]
\caption{Something about the quality of results in the
  cross-validation as a function of the signal-to-noise of what we
  give in the test subset.\label{fig:snr}}
\end{figure}

\end{document}
